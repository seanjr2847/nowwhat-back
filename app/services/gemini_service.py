import json
import asyncio
from typing import List, Dict, Optional
import google.generativeai as genai
from app.core.config import settings
from app.schemas.nowwhat import IntentOption
from app.schemas.questions import Question, Option
from app.prompts.prompt_selector import (
    get_intent_analysis_prompt, get_questions_generation_prompt, get_checklist_generation_prompt,
    get_intent_analysis_response_class, get_questions_list_response_class, get_checklist_response_class
)
from app.prompts.search_prompts import get_search_prompt, SearchResponse
from app.prompts.enhanced_prompts import get_enhanced_knowledge_prompt
import logging
import uuid
from dataclasses import dataclass
from typing import Any
from pydantic import BaseModel

# Constants
class GeminiConfig:
    """Gemini Service Configuration Constants"""
    RETRY_ATTEMPTS = 3
    EXPECTED_INTENTS_COUNT = 4
    MIN_CONTENT_LENGTH = 50
    CHUNK_SIZE = 100
    STREAM_DELAY = 0.01
    MAX_OUTPUT_TOKENS = 20480
    TEMPERATURE = 0.7
    TOP_P = 0.8
    TOP_K = 40
    TIMEOUT_SECONDS = 30
    CONCURRENT_SEARCH_LIMIT = 15

class GeminiServiceError(Exception):
    """Base exception for Gemini Service errors"""
    pass

class GeminiAPIError(GeminiServiceError):
    """Exception raised for Gemini API errors"""
    pass

class GeminiResponseError(GeminiServiceError):
    """Exception raised for response parsing errors"""
    pass

logger = logging.getLogger(__name__)
# Gemini ì„œë¹„ìŠ¤ ë””ë²„ê¹…ì„ ìœ„í•´ ì„ì‹œë¡œ DEBUG ë ˆë²¨ ì„¤ì •
logger.setLevel(logging.DEBUG)

# Pydantic ëª¨ë¸ë“¤ì€ ì´ì œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì—ì„œ import

@dataclass
class SearchResult:
    """Gemini API ê²€ìƒ‰ ê²°ê³¼"""
    query: str
    content: str
    sources: List[str]
    success: bool
    error_message: Optional[str] = None
# ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€ (ì´ë¯¸ ìˆë‹¤ë©´ ë¬´ì‹œë¨)
if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

# Gemini API ì„¤ì •
if settings.GEMINI_API_KEY:
    genai.configure(api_key=settings.GEMINI_API_KEY)
    logger.info(f"Gemini API configured with key: {settings.GEMINI_API_KEY[:10]}...{settings.GEMINI_API_KEY[-4:]}")
else:
    logger.error("GEMINI_API_KEY not found in settings")

class GeminiService:
    def __init__(self):
        if not settings.GEMINI_API_KEY:
            logger.error("Cannot initialize GeminiService: GEMINI_API_KEY not set")
            raise ValueError("GEMINI_API_KEY not configured")
        
        self.model = genai.GenerativeModel(settings.GEMINI_MODEL)
        logger.info(f"GeminiService initialized with model: {settings.GEMINI_MODEL}")
        
    async def analyze_intent(self, goal: str, country_info: str = "", language_info: str = "", country_option: bool = True) -> List[IntentOption]:
        """ì‚¬ìš©ì ëª©í‘œ ë¶„ì„ ë° ì˜ë„ ì˜µì…˜ ìƒì„±
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ì‚¬ìš©ì ì…ë ¥ ëª©í‘œë¥¼ Gemini AIë¡œ ë¶„ì„í•˜ì—¬ 4ê°€ì§€ êµ¬ì²´ì ì¸ ì‹¤í–‰ ì˜ë„ ë„ì¶œ
        - ì§€ì—­ì •ë³´(country_info)ì™€ ì–¸ì–´ì •ë³´(language_info)ë¥¼ í™œìš©í•œ ë§ì¶¤í˜• ì˜ë„ ìƒì„±
        - 3íšŒ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ API ì‹¤íŒ¨ ì‹œ ì•ˆì •ì„± ë³´ì¥
        - API ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í…œí”Œë¦¿ìœ¼ë¡œ í´ë°±í•˜ì—¬ ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ë°©ì§€
        """
        try:
            user_language = self._extract_user_language(language_info)
            prompt = self._create_prompt(goal, country_info, language_info, user_language, country_option)
            
            intents = await self._analyze_intent_with_retry(prompt)
            return intents if intents else self._get_default_template()
            
        except Exception as e:
            logger.error(f"Intent analysis failed: {str(e)}")
            return self._get_default_template()

    async def _analyze_intent_with_retry(self, prompt: str) -> Optional[List[IntentOption]]:
        """ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•œ ì•ˆì •ì ì¸ ì˜ë„ ë¶„ì„
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Gemini API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ìµœëŒ€ 3íšŒ ì¬ì‹œë„í•˜ì—¬ ì¼ì‹œì  ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ê·¹ë³µ
        - ê° ì¬ì‹œë„ ê°„ ì§€ìˆ˜ë°±ì˜¤í”„ ì ìš©ìœ¼ë¡œ ì„œë²„ ë¶€í•˜ ìµœì†Œí™”
        - êµ¬ì¡°í™”ëœ ì¶œë ¥ í˜•ì‹ìœ¼ë¡œ ì¼ê´€ëœ ì‘ë‹µ ë°ì´í„° ë³´ì¥
        - íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ ì¬ì‹œë„í•˜ì—¬ ë°ì´í„° ë¬´ê²°ì„± í™•ë³´
        """
        for attempt in range(GeminiConfig.RETRY_ATTEMPTS):
            try:
                response = await self._call_gemini_api(prompt)
                intents = self._parse_response(response)
                
                if len(intents) == GeminiConfig.EXPECTED_INTENTS_COUNT:
                    return intents
                else:
                    logger.warning(f"Gemini returned {len(intents)} intents instead of {GeminiConfig.EXPECTED_INTENTS_COUNT} (attempt {attempt + 1})")
                    
            except Exception as e:
                logger.error(f"Gemini API call failed (attempt {attempt + 1}): {str(e)}")
                if attempt < GeminiConfig.RETRY_ATTEMPTS - 1:
                    await asyncio.sleep(2 ** attempt)  # exponential backoff
                
        logger.warning("All Gemini API attempts failed, using default template")
        return None

    async def generate_questions(
        self, 
        goal: str, 
        intent_title: str, 
        user_country: Optional[str] = None,
        user_language: Optional[str] = None,
        country_option: bool = True
    ) -> List[Question]:
        """ì„ íƒëœ ì˜ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„±
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ì‚¬ìš©ìì˜ ëª©í‘œì™€ ì„ íƒëœ ì˜ë„ì— ë”°ë¼ 3-5ê°œì˜ ìƒì„¸ ì§ˆë¬¸ ìƒì„±
        - ì§€ì—­ì •ë³´ì™€ ì–¸ì–´ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë³¸í™”ëœ ì§ˆë¬¸ ì œê³µ
        - ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹¤ì¤‘ ì„ íƒ ì˜µì…˜ë„ í•¨ê»˜ ìƒì„±
        - API ì‹¤íŒ¨ ì‹œ ë²”ìš© ì§ˆë¬¸ í…œí”Œë¦¿ìœ¼ë¡œ í´ë°±í•˜ì—¬ ì„œë¹„ìŠ¤ ì—°ì†ì„± ë³´ì¥
        """
        try:
            prompt = self._create_questions_prompt(goal, intent_title, user_country, user_language, country_option)
            questions = await self._generate_questions_with_retry(prompt, intent_title)
            return questions if questions else self._get_cached_questions_template(intent_title)
            
        except Exception as e:
            logger.error(f"Question generation failed: {str(e)}")
            return self._get_cached_questions_template(intent_title)

    async def _generate_questions_with_retry(self, prompt: str, intent_title: str) -> Optional[List[Question]]:
        """ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•œ ì•ˆì •ì ì¸ ì§ˆë¬¸ ìƒì„±
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Gemini APIë¡œ ì§ˆë¬¸ ìƒì„± ì‹œ ìµœëŒ€ 3íšŒ ì¬ì‹œë„ë¡œ ì•ˆì •ì„± í™•ë³´
        - API ì‹¤íŒ¨ ë˜ëŠ” ë¬´íš¨í•œ ì‘ë‹µ ì‹œ ì§€ìˆ˜ë°±ì˜¤í”„ë¡œ ì¬ì‹œë„ ì£¼ê¸° ì¡°ì ˆ
        - ê° ì‹œë„ì—ì„œ ì§ˆë¬¸ ìˆ˜ì™€ êµ¬ì¡° ìœ íš¨ì„± ê²€ì¦
        - ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ìºì‹œëœ í…œí”Œë¦¿ìœ¼ë¡œ í´ë°±
        """
        for attempt in range(GeminiConfig.RETRY_ATTEMPTS):
            try:
                response = await self._call_gemini_api(prompt)
                questions = self._parse_questions_response(response)
                
                logger.info(f"Gemini returned {len(questions)} questions")
                return questions
                    
            except Exception as e:
                logger.error(f"Gemini questions API call failed (attempt {attempt + 1}): {str(e)}")
                if attempt < GeminiConfig.RETRY_ATTEMPTS - 1:
                    await asyncio.sleep(2 ** attempt)
                
        logger.warning("All Gemini questions API attempts failed, using cached template")
        return None

    async def generate_questions_stream(
        self, 
        goal: str, 
        intent_title: str, 
        user_country: Optional[str] = None,
        user_language: Optional[str] = None,
        country_option: bool = True
    ):
        """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„±
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Server-Sent Events (SSE) í˜•ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ì§ˆë¬¸ ìƒì„± ê³¼ì •ì„ ì‚¬ìš©ìì—ê²Œ ì „ì†¡
        - ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ë°ì´í„° ë¬´ê²°ì„± ì‹¤ì‹œê°„ ê²€ì¦
        - JSON ë°ì´í„° ì™„ì „ì„± ê²€ì¦ ë° ë¶ˆì™„ì „ ì‹œ ìë™ ìˆ˜ì •
        - ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ í´ë°± ì§ˆë¬¸ ìƒì„±ìœ¼ë¡œ ì‚¬ìš©ì ê²½í—˜ ë³´ì¥
        - ì–´ë–¤ ìƒí™©ì—ì„œë„ ì‚¬ìš©ìëŠ” í•­ìƒ ì™„ì „í•œ ë°ì´í„° ìˆ˜ì‹ 
        """
        stream_id = str(uuid.uuid4())[:8]
        accumulated_content = ""
        
        try:
            prompt = self._create_questions_prompt(goal, intent_title, user_country, user_language, country_option)
            logger.info(f"ğŸŒŠ Starting streaming question generation for: {goal} (intent: {intent_title}) [Stream: {stream_id}]")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ë° ê²°ê³¼ ì²˜ë¦¬
            async for chunk in self._stream_questions_with_validation(prompt, stream_id, accumulated_content):
                accumulated_content += chunk
                yield chunk
            
            # í›„ì²˜ë¦¬: ì™„ì „ì„± ê²€ì¦ ë° í´ë°± ì²˜ë¦¬
            async for chunk in self._handle_stream_completion(
                accumulated_content, stream_id, goal, intent_title, user_country, user_language, country_option
            ):
                yield chunk
                
        except Exception as e:
            logger.error(f"ğŸš¨ Streaming question generation failed [Stream: {stream_id}]: {str(e)}")
            async for chunk in self._handle_stream_error(stream_id, intent_title):
                yield chunk

    async def _stream_questions_with_validation(self, prompt: str, stream_id: str, accumulated_content: str):
        """ì‹¤ì‹œê°„ ê²€ì¦ì´ í¬í•¨ëœ ì§ˆë¬¸ ìŠ¤íŠ¸ë¦¬ë°
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Gemini API ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²­í¬ ë‹¨ìœ„ë¡œ ìˆ˜ì‹ 
        - ê° ì²­í¬ë§ˆë‹¤ JSON êµ¬ì¡° ìœ íš¨ì„± ì˜ˆë¹„ ê²€ì¦
        - ìŠ¤íŠ¸ë¦¬ë° ì¤‘ë‹¨ ë˜ëŠ” ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¦‰ì‹œ ê°ì§€
        - ëˆ„ì ëœ ì½˜í…ì¸ ì˜ ì™„ì „ì„±ì„ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
        """
        async for chunk in self._call_gemini_api_stream_with_validation(prompt, stream_id):
            yield chunk

    async def _handle_stream_completion(self, content: str, stream_id: str, goal: str, intent_title: str, 
                                      user_country: Optional[str], user_language: Optional[str], country_option: bool):
        """ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ í›„ ë°ì´í„° ì™„ì „ì„± ì²˜ë¦¬
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹œ ëˆ„ì ëœ ì½˜í…ì¸ ì˜ JSON êµ¬ì¡° ì™„ì „ì„± ê²€ì¦
        - ë¶ˆì™„ì „í•œ JSON ê°ì§€ ì‹œ ì¦‰ì‹œ í´ë°± ì§ˆë¬¸ ìƒì„±ìœ¼ë¡œ ëŒ€ì²´
        - ì‚¬ìš©ìê°€ í•­ìƒ ì™„ì „í•œ ë°ì´í„°ë¥¼ ë°›ë„ë¡ ë³´ì¥
        - í´ë°± ìƒì„± ì‹œ ë™ì¼í•œ ë§¤ê°œë³€ìˆ˜ë¡œ ë§¥ë½ ì¼ê´€ì„± ìœ ì§€
        """
        if not self._validate_json_completeness(content, stream_id):
            logger.warning(f"ğŸš¨ Incomplete JSON detected [Stream: {stream_id}], generating fallback")
            fallback_content = await self._generate_fallback_questions(goal, intent_title, user_country, user_language, country_option)
            if fallback_content:
                yield "\n\n--- ì™„ì „í•œ ì§ˆë¬¸ ë°ì´í„° ---\n"
                yield fallback_content

    async def _handle_stream_error(self, stream_id: str, intent_title: str):
        """ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ì‹œ ë¹„ìƒ ëŒ€ìœµ ì²˜ë¦¬
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ìŠ¤íŠ¸ë¦¬ë° API ì‹¤íŒ¨ ì‹œ ì§†ì‹œ ì—†ì´ ëŒ€ì•ˆ ë°ì´í„° ì œê³µ
        - ì˜ë„ë³„ ìºì‹œëœ ì§ˆë¬¸ í…œí”Œë¦¿ì„ ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹ìœ¼ë¡œ ì „ì†¡
        - ì²­í¬ ë‹¨ìœ„ ì „ì†¨ìœ¼ë¡œ ì‹¤ì‹œê°„ ì „ì†¡ íš¨ê³¼ ìœ ì§€
        - í´ë°± ë°ì´í„°ë„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì˜¤ë¥˜ ë©”ì‹œì§€ ëŒ€ì‹  JSON í˜•íƒœë¡œ ì œê³µ
        """
        try:
            cached_questions = self._get_cached_questions_template(intent_title)
            questions_json = json.dumps({"questions": [q.dict() for q in cached_questions]}, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“¦ Sending cached questions [Stream: {stream_id}], size: {len(questions_json)} chars")
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ì „ì†¡
            for i in range(0, len(questions_json), GeminiConfig.CHUNK_SIZE):
                chunk = questions_json[i:i + GeminiConfig.CHUNK_SIZE]
                yield chunk
                await asyncio.sleep(GeminiConfig.STREAM_DELAY)
                
        except Exception as fallback_error:
            logger.error(f"ğŸš¨ Fallback generation also failed [Stream: {stream_id}]: {str(fallback_error)}")
            yield '{"error": "ì§ˆë¬¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}'

    def _create_questions_prompt(self, goal: str, intent_title: str, user_country: Optional[str] = None, user_language: Optional[str] = None, country_option: bool = True) -> str:
        """ì§ˆë¬¸ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        country_context = self._get_country_context(user_country)
        language_context = self._get_language_context(user_language)
        
        return get_questions_generation_prompt(
            goal=goal,
            intent_title=intent_title,
            user_country=user_country or "ì •ë³´ ì—†ìŒ",
            user_language=user_language or "ì •ë³´ ì—†ìŒ",
            country_context=country_context,
            language_context=language_context,
            country_option=country_option
        )

    def _get_country_context(self, user_country: Optional[str]) -> str:
        """
        êµ­ê°€ë³„ ë§ì¶¤ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            user_country: ì‚¬ìš©ì êµ­ê°€ ì½”ë“œ (ISO 2ìë¦¬)
            
        Returns:
            í•´ë‹¹ êµ­ê°€ì˜ ë¬¸í™”ì  ë§¥ë½ì„ ì„¤ëª…í•˜ëŠ” ë¬¸ìì—´
        """
        contexts: Dict[str, str] = {
            "KR": "í•œêµ­ ê±°ì£¼ì ê¸°ì¤€, í•œêµ­ ë¬¸í™”ì™€ í™˜ê²½ ê³ ë ¤",
            "US": "ë¯¸êµ­ ê±°ì£¼ì ê¸°ì¤€, ë¯¸êµ­ ë¬¸í™”ì™€ í™˜ê²½ ê³ ë ¤", 
            "JP": "ì¼ë³¸ ê±°ì£¼ì ê¸°ì¤€, ì¼ë³¸ ë¬¸í™”ì™€ í™˜ê²½ ê³ ë ¤",
            "CN": "ì¤‘êµ­ ê±°ì£¼ì ê¸°ì¤€, ì¤‘êµ­ ë¬¸í™”ì™€ í™˜ê²½ ê³ ë ¤"
        }
        return contexts.get(user_country, "ê¸€ë¡œë²Œ ê¸°ì¤€")

    def _get_language_context(self, user_language: Optional[str]) -> str:
        """
        ì–¸ì–´ë³„ ë§ì¶¤ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            user_language: ì‚¬ìš©ì ì–¸ì–´ ì½”ë“œ (ISO 639-1)
            
        Returns:
            í•´ë‹¹ ì–¸ì–´ì˜ ë¬¸í™”ì  ë§¥ë½ì„ ì„¤ëª…í•˜ëŠ” ë¬¸ìì—´
        """
        contexts: Dict[str, str] = {
            "ko": "í•œêµ­ì–´ ê¸°ì¤€, í•œêµ­ ë¬¸í™”ì  ë§¥ë½ ê³ ë ¤",
            "en": "English, Western cultural context",
            "ja": "æ—¥æœ¬èªã€æ—¥æœ¬ã®æ–‡åŒ–çš„æ–‡è„ˆã‚’è€ƒæ…®",
            "zh": "ä¸­æ–‡ï¼Œä¸­å›½æ–‡åŒ–èƒŒæ™¯è€ƒè™‘",
            "es": "EspaÃ±ol, contexto cultural hispano",
            "fr": "FranÃ§ais, contexte culturel franÃ§ais"
        }
        return contexts.get(user_language, "ë‹¤êµ­ì–´ ì§€ì›")

    def _parse_questions_response(self, response: str) -> List[Question]:
        """Gemini API ì§ˆë¬¸ ìƒì„± ì‘ë‹µ íŒŒì‹±
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ê°¤ë¯¸ë‹ˆì—ì„œ ìˆ˜ì‹ í•œ JSON í˜•íƒœì˜ ì§ˆë¬¸ ë°ì´í„°ë¥¼ Question ê°ì²´ë¡œ ë³€í™˜
        - ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ë‚´ì—ì„œ JSON ë°ì´í„° ì¶”ì¶œ
        - ì§ˆë¬¸ êµ¬ì¡° ë° í•„ìˆ˜ í•„ë“œ ìœ íš¨ì„± ê²€ì¦
        - íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒìœ¼ë¡œ ìƒìœ„ ë ˆì´ì–´ì— ì˜¤ë¥˜ ì „íŒŒ
        """
        try:
            # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if not response or not response.strip():
                logger.warning("Gemini returned empty response")
                raise ValueError("Empty response from Gemini")
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            response = response.strip()
            logger.debug(f"Raw Gemini response: {response[:200]}...")  # ë””ë²„ê¹…ìš©
            
            if response.startswith("```json"):
                response = response[7:]
            if response.endswith("```"):
                response = response[:-3]
            response = response.strip()
            
            # ë¹ˆ ì‘ë‹µ ì¬í™•ì¸
            if not response:
                logger.warning("Response became empty after cleaning")
                raise ValueError("Empty response after cleaning")
            
            questions_data = json.loads(response)
            
            if not isinstance(questions_data, list):
                raise ValueError("Response is not a list")
                
            questions = []
            for item in questions_data:
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                if not all(key in item for key in ["id", "text", "type", "required"]):
                    raise ValueError("Missing required fields in question")
                
                # options ì²˜ë¦¬
                options = None
                if item["type"] == "multiple" and "options" in item:
                    options = [
                        Option(
                            id=opt["id"],
                            text=opt["text"], 
                            value=opt["value"]
                        ) for opt in item["options"]
                    ]
                
                questions.append(Question(
                    id=item["id"],
                    text=item["text"],
                    type=item["type"],
                    options=options,
                    required=item["required"]
                ))
                
            return questions
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {str(e)}, Response: '{response[:500]}'")
            raise Exception(f"Failed to parse Gemini questions response: Invalid JSON - {str(e)}")
        except Exception as e:
            logger.error(f"Questions parsing error: {str(e)}, Response: '{response[:500] if 'response' in locals() else 'N/A'}'")
            raise Exception(f"Failed to parse Gemini questions response: {str(e)}")

    def _get_cached_questions_template(self, intent_title: str) -> List[Question]:
        """ì˜ë„ë³„ í´ë°± ì§ˆë¬¸ í…œí”Œë¦¿ ì œê³µ
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Gemini API ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ì˜ë„ë³„ ê¸°ë³¸ ì§ˆë¬¸ í…œí”Œë¦¿ ì œê³µ
        - ì£¼ìš” ì˜ë„(ì—¬í–‰, ê±´ê°•, ê°œë°œ, ìê¸°ê³„ë°œ)ì— ëŒ€í•´ ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸ ì„¸íŠ¸
        - ê° ì§ˆë¬¸ì€ ë‹¤ì¤‘ ì„ íƒ í˜•íƒœë¡œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ì¡°
        - ì§€ì›ë˜ì§€ ì•ŠëŠ” ì˜ë„ì— ëŒ€í•´ì„œë„ ë²”ìš© ì§ˆë¬¸ ì œê³µ
        """
        templates = {
            "ì—¬í–‰ ê³„íš": [
                Question(
                    id="q_duration",
                    text="ì—¬í–‰ ê¸°ê°„ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?",
                    type="multiple",
                    options=[
                        Option(id="opt_3days", text="2ë°• 3ì¼", value="3days"),
                        Option(id="opt_5days", text="4ë°• 5ì¼", value="5days"),
                        Option(id="opt_1week", text="1ì£¼ì¼", value="1week"),
                        Option(id="opt_longer", text="1ì£¼ì¼ ì´ìƒ", value="longer")
                    ],
                    required=True
                ),
                Question(
                    id="q_companions",
                    text="ëˆ„êµ¬ì™€ í•¨ê»˜ ê°€ì‹œë‚˜ìš”?",
                    type="multiple",
                    options=[
                        Option(id="opt_alone", text="í˜¼ì", value="alone"),
                        Option(id="opt_couple", text="ì—°ì¸/ë°°ìš°ìì™€", value="couple"),
                        Option(id="opt_family", text="ê°€ì¡±ê³¼", value="family"),
                        Option(id="opt_friends", text="ì¹œêµ¬ë“¤ê³¼", value="friends")
                    ],
                    required=True
                ),
                Question(
                    id="q_activities",
                    text="ì£¼ë¡œ í•˜ê³  ì‹¶ì€ í™œë™ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    type="multiple",
                    options=[
                        Option(id="opt_sightseeing", text="ê´€ê´‘/ëª…ì†Œ íƒë°©", value="sightseeing"),
                        Option(id="opt_food", text="ë§›ì§‘ íƒë°©", value="food"),
                        Option(id="opt_shopping", text="ì‡¼í•‘", value="shopping"),
                        Option(id="opt_culture", text="ë¬¸í™” ì²´í—˜", value="culture")
                    ],
                    required=True
                )
            ],
            "ê³„íš ì„¸ìš°ê¸°": [
                Question(
                    id="q_timeline",
                    text="ì–¸ì œê¹Œì§€ ê³„íšì„ ì™„ì„±í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                    type="multiple",
                    options=[
                        Option(id="opt_1week", text="1ì£¼ì¼ ë‚´", value="1week"),
                        Option(id="opt_1month", text="1ë‹¬ ë‚´", value="1month"),
                        Option(id="opt_3months", text="3ë‹¬ ë‚´", value="3months"),
                        Option(id="opt_flexible", text="ìœ ì—°í•˜ê²Œ", value="flexible")
                    ],
                    required=True
                ),
                Question(
                    id="q_priority",
                    text="ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê²ƒì€?",
                    type="multiple",
                    options=[
                        Option(id="opt_time", text="ì‹œê°„ íš¨ìœ¨ì„±", value="time"),
                        Option(id="opt_cost", text="ë¹„ìš© ì ˆì•½", value="cost"),
                        Option(id="opt_quality", text="í’ˆì§ˆ/ë§Œì¡±ë„", value="quality"),
                        Option(id="opt_convenience", text="í¸ì˜ì„±", value="convenience")
                    ],
                    required=True
                )
            ]
        }
        
        return templates.get(intent_title, self._get_default_questions_template())

    def _get_default_questions_template(self) -> List[Question]:
        """ê¸°ë³¸ ì§ˆë¬¸ í…œí”Œë¦¿"""
        return [
            Question(
                id="q_when",
                text="ì–¸ì œê¹Œì§€ ì´ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                type="multiple",
                options=[
                    Option(id="opt_1week", text="1ì£¼ì¼ ë‚´", value="1week"),
                    Option(id="opt_1month", text="1ë‹¬ ë‚´", value="1month"),
                    Option(id="opt_3months", text="3ë‹¬ ë‚´", value="3months"),
                    Option(id="opt_6months", text="6ë‹¬ ë‚´", value="6months")
                ],
                required=True
            ),
            Question(
                id="q_resources",
                text="ì´ ëª©í‘œë¥¼ ìœ„í•´ íˆ¬ìí•  ìˆ˜ ìˆëŠ” ìì›ì€?",
                type="multiple", 
                options=[
                    Option(id="opt_time", text="ì£¼ë¡œ ì‹œê°„", value="time"),
                    Option(id="opt_money", text="ì£¼ë¡œ ëˆ", value="money"),
                    Option(id="opt_both", text="ì‹œê°„ê³¼ ëˆ ëª¨ë‘", value="both"),
                    Option(id="opt_minimal", text="ìµœì†Œí•œë§Œ", value="minimal")
                ],
                required=True
            ),
            Question(
                id="q_priority",
                text="ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê²ƒì€?",
                type="multiple",
                options=[
                    Option(id="opt_speed", text="ë¹ ë¥¸ ë‹¬ì„±", value="speed"),
                    Option(id="opt_quality", text="ë†’ì€ í’ˆì§ˆ", value="quality"),
                    Option(id="opt_cost", text="ë¹„ìš© ì ˆì•½", value="cost"),
                    Option(id="opt_balance", text="ê· í˜•ì¡íŒ ì ‘ê·¼", value="balance")
                ],
                required=True
            )
        ]
    
    def _create_prompt(self, goal: str, country_info: str = "", language_info: str = "", user_language: str = None, country_option: bool = True) -> str:
        """Gemini APIìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return get_intent_analysis_prompt(goal, country_info, language_info, user_language, country_option)
    
    def _extract_user_language(self, language_info: str) -> str:
        """language_info ë¬¸ìì—´ì—ì„œ ì‚¬ìš©ì ì–¸ì–´ ì¶”ì¶œ"""
        if not language_info:
            return None
        
        # "ì‚¬ìš©ì ì–¸ì–´: ko" í˜•íƒœì—ì„œ ì–¸ì–´ ì½”ë“œ ì¶”ì¶œ
        if ":" in language_info:
            return language_info.split(":")[-1].strip()
        
        return language_info.strip()

    async def _call_gemini_api_with_search(self, prompt: str) -> str:
        """Gemini API í˜¸ì¶œ (ê³µì‹ Google Search ê¸°ëŠ¥ ì‚¬ìš©)"""
        try:
            logger.debug(f"Sending search prompt to Gemini (length: {len(prompt)} chars)")
            
            # ê³µì‹ Google Search grounding êµ¬í˜„ + Structured Output
            try:
                # Google Search ë„êµ¬ ì„¤ì • (ê³µì‹ ë°©ë²•)
                search_tool = genai.protos.Tool(
                    google_search_retrieval=genai.protos.GoogleSearchRetrieval()
                )
                
                logger.debug("Using Google Search grounding tool with structured output")
                
                # SearchResponseë¥¼ Gemini í˜¸í™˜ ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
                response_schema = self._create_gemini_compatible_schema()
                
                response = await asyncio.to_thread(
                    self.model.generate_content,
                    prompt,
                    tools=[search_tool],
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=8192,  # ê²€ìƒ‰ìš©ìœ¼ë¡œ ë” ì‘ì€ í† í° ì‚¬ìš©
                        temperature=GeminiConfig.TEMPERATURE,
                        top_p=GeminiConfig.TOP_P,
                        top_k=GeminiConfig.TOP_K,
                        response_mime_type="application/json",
                        response_schema=response_schema
                    )
                )
                
                logger.debug("Google Search grounding with structured output completed")
                
            except Exception as tool_error:
                logger.warning(f"Google Search grounding failed: {tool_error}")
                logger.debug(f"Error type: {type(tool_error).__name__}")
                logger.info("Trying alternative Google Search implementation")
                
                try:
                    # ëŒ€ì•ˆì  êµ¬í˜„ (ìµœì‹  SDK) + Structured Output
                    from google.generativeai.types import Tool
                    
                    # ìµœì‹  SDKì˜ GoogleSearch ë„êµ¬
                    search_tool = Tool(
                        google_search_retrieval={}
                    )
                    
                    # Gemini í˜¸í™˜ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
                    response_schema = self._create_gemini_compatible_schema()
                    
                    response = await asyncio.to_thread(
                        self.model.generate_content,
                        prompt,
                        tools=[search_tool],
                        generation_config=genai.types.GenerationConfig(
                            max_output_tokens=16384,  # ê²€ìƒ‰ ê¸°ëŠ¥ í† í° ì œí•œ ì¦ê°€
                            temperature=0.7,
                            top_p=0.8,
                            top_k=40,
                            response_mime_type="application/json",
                            response_schema=response_schema
                        )
                    )
                    
                except Exception as alt_error:
                    logger.warning(f"Alternative Google Search failed: {alt_error}")
                    logger.debug(f"Alternative error type: {type(alt_error).__name__}")
                    logger.info("Using enhanced knowledge-based prompt")
                    
                    # ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, ìµœì‹  ì •ë³´ ìš”ì²­ í”„ë¡¬í”„íŠ¸ + Structured Output
                    enhanced_prompt = get_enhanced_knowledge_prompt(prompt)

                    # Gemini í˜¸í™˜ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
                    response_schema = self._create_gemini_compatible_schema()
                    
                    response = await asyncio.to_thread(
                        self.model.generate_content,
                        enhanced_prompt,
                        generation_config=genai.types.GenerationConfig(
                            max_output_tokens=16384,  # ê²€ìƒ‰ ê¸°ëŠ¥ í† í° ì œí•œ ì¦ê°€
                            temperature=0.7,
                            top_p=0.8,
                            top_k=40,
                            response_mime_type="application/json",
                            response_schema=response_schema
                        )
                    )
            
            # ì‘ë‹µ ì²˜ë¦¬
            if not response:
                logger.error("Gemini returned None response")
                raise Exception("Gemini returned None response")
            
            # grounding metadata í™•ì¸ (ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€)
            if hasattr(response, 'candidates') and response.candidates:
                for candidate in response.candidates:
                    if hasattr(candidate, 'grounding_metadata'):
                        logger.info("Response includes grounding metadata (web search results)")
                        if hasattr(candidate.grounding_metadata, 'search_entry_point'):
                            logger.debug(f"Search entry point: {candidate.grounding_metadata.search_entry_point}")
                        if hasattr(candidate.grounding_metadata, 'grounding_chunks'):
                            logger.debug(f"Found {len(candidate.grounding_metadata.grounding_chunks)} grounding chunks")
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if not hasattr(response, 'text'):
                logger.debug("Extracting text from candidates...")
                if hasattr(response, 'candidates') and response.candidates:
                    for candidate in response.candidates:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    response_text = part.text
                                    break
                    else:
                        raise Exception("No text found in candidates")
                else:
                    raise Exception("No candidates in response")
            else:
                response_text = response.text
            
            logger.debug(f"Gemini search response received (length: {len(response_text) if response_text else 0})")
            
            if not response_text or not response_text.strip():
                logger.error("Gemini returned empty response")
                raise Exception("Gemini returned empty response")
                
            return response_text
            
        except Exception as e:
            logger.error(f"Gemini search API call error: {str(e)}")
            logger.debug(f"Final error type: {type(e).__name__}")
            # ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨ì‹œ ì¼ë°˜ APIë¡œ í´ë°±
            logger.info("Falling back to regular Gemini API without search")
            return await self._call_gemini_api(prompt)
    
    async def _call_gemini_api(self, prompt: str) -> str:
        """Gemini API ì¼ë°˜ í˜¸ì¶œ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ë™ê¸°ì‹ Gemini API í˜¸ì¶œë¡œ ì „ì²´ ì‘ë‹µì„ í•œ ë²ˆì— ìˆ˜ì‹ 
        - ìƒì„± ì½˜í”¼ê·¸ ì„¤ì •ìœ¼ë¡œ ìŒì„±ì˜ ë‹¤ì–‘ì„±ê³¼ í’ˆì§ˆ ì œì–´
        - ì‘ë‹µ êµ¬ì¡° ë° Safety Rating ìƒì„¸ ê²€ì¦
        - ë¹ˆ ì‘ë‹µ ë˜ëŠ” ë§¤ì„œë“œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
        """
        try:
            logger.debug(f"Sending prompt to Gemini (length: {len(prompt)} chars)")
            response = await asyncio.to_thread(
                self.model.generate_content, 
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=16384,  # ì§ˆë¬¸ ìƒì„± í† í° ì œí•œ ì¦ê°€
                    temperature=0.7,
                    top_p=0.8,
                    top_k=40
                )
            )
            
            # ì‘ë‹µ ìƒíƒœ í™•ì¸
            if not response:
                logger.error("Gemini returned None response")
                raise Exception("Gemini returned None response")
            
            # ì‘ë‹µ ê°ì²´ êµ¬ì¡° í™•ì¸
            logger.debug(f"Gemini response type: {type(response)}")
            logger.debug(f"Gemini response attributes: {dir(response)}")
            
            # Safety rating ë° finish reason í™•ì¸
            if hasattr(response, 'candidates') and response.candidates:
                for i, candidate in enumerate(response.candidates):
                    finish_reason = candidate.finish_reason if hasattr(candidate, 'finish_reason') else 'N/A'
                    logger.debug(f"Candidate {i} finish_reason: {finish_reason}")
                    
                    # finish_reason í•´ì„
                    if finish_reason == 2:
                        logger.warning("Response was truncated due to MAX_TOKENS limit - consider increasing max_output_tokens")
                    elif finish_reason == 3:
                        logger.warning("Response was blocked by safety filters")
                    elif finish_reason == 4:
                        logger.warning("Response was blocked due to recitation concerns")
                    
                    if hasattr(candidate, 'safety_ratings'):
                        logger.debug(f"Candidate {i} safety_ratings: {candidate.safety_ratings}")
            
            if not hasattr(response, 'text'):
                logger.error(f"Gemini response has no text attribute: {type(response)}")
                # ëŒ€ì•ˆìœ¼ë¡œ candidatesì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                if hasattr(response, 'candidates') and response.candidates:
                    for candidate in response.candidates:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    logger.debug(f"Found text in candidate.content.parts: {part.text[:100]}...")
                                    response_text = part.text
                                    break
                    else:
                        raise Exception("Gemini response has no text attribute and no text in candidates")
                else:
                    raise Exception("Gemini response has no text attribute")
            else:
                response_text = response.text
            
            logger.debug(f"Raw Gemini response (length: {len(response_text) if response_text else 0}): '{response_text}'")
            
            if not response_text or not response_text.strip():
                logger.error("Gemini returned empty or whitespace-only response")
                raise Exception("Gemini returned empty response")
                
            return response_text
            
        except Exception as e:
            logger.error(f"Gemini API call error details: {str(e)}")
            raise Exception(f"Gemini API call failed: {str(e)}")
    
    async def _call_gemini_api_stream(self, prompt: str):
        """Gemini API ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Server-Sent Events í˜•ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ë°ì´í„° ì „ì†¡
        - generator_content_stream() í•¨ìˆ˜ë¡œ ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ìˆ˜ì‹ 
        - ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì§„ë‹¨ ì •ë³´ì™€ í•¨ê»˜ ì˜ˆì™¸ ë°œìƒ
        - ê° ì²­í¬ì— ëŒ€í•œ ë¡œê¹… ë° ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨
        """
        try:
            logger.debug(f"Starting streaming request to Gemini (prompt length: {len(prompt)} chars)")
            
            # Gemini ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •
            generation_config = genai.types.GenerationConfig(
                max_output_tokens=16384,  # ì§ˆë¬¸ ìƒì„± í† í° ì œí•œ ì¦ê°€
                temperature=0.7,
                top_p=0.8,
                top_k=40
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            response_stream = await asyncio.to_thread(
                self.model.generate_content,
                prompt,
                generation_config=generation_config,
                stream=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )
            
            logger.debug("Gemini streaming response initiated")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            for chunk in response_stream:
                if hasattr(chunk, 'text') and chunk.text:
                    yield chunk.text
                elif hasattr(chunk, 'candidates') and chunk.candidates:
                    for candidate in chunk.candidates:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    yield part.text
                                    
        except Exception as e:
            logger.error(f"Gemini streaming API error: {str(e)}")
            logger.debug(f"Error type: {type(e).__name__}")
            raise Exception(f"Gemini streaming API failed: {str(e)}")
    
    def _parse_response(self, response: str) -> List[IntentOption]:
        """Gemini ì‘ë‹µ íŒŒì‹±"""
        try:
            logger.debug(f"Intent parsing - Raw response: '{response[:200]}...' (total length: {len(response)})")
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
            response = response.strip()
            logger.debug(f"Intent parsing - After strip: '{response[:200]}...'")
            
            if response.startswith("```json"):
                response = response[7:]
                logger.debug("Intent parsing - Removed ```json prefix")
            if response.endswith("```"):
                response = response[:-3]
                logger.debug("Intent parsing - Removed ``` suffix")
            response = response.strip()
            
            logger.debug(f"Intent parsing - Final JSON to parse: '{response[:200]}...'")
            
            if not response:
                logger.error("Intent parsing - Response became empty after cleaning")
                raise ValueError("Empty response after cleaning")
            
            json_data = json.loads(response)
            logger.debug(f"Intent parsing - Successfully parsed JSON: {type(json_data)}")
            
            # JSON ì‘ë‹µì´ {"intents": [...]} í˜•íƒœì¸ì§€ í™•ì¸
            if isinstance(json_data, dict) and "intents" in json_data:
                intents_data = json_data["intents"]
                logger.debug(f"Intent parsing - Found intents array with {len(intents_data)} items")
            elif isinstance(json_data, list):
                intents_data = json_data
                logger.debug(f"Intent parsing - Using direct list with {len(intents_data)} items")
            else:
                raise ValueError(f"Unexpected response format: {type(json_data)}")
                
            intents = []
            for item in intents_data:
                if not all(key in item for key in ["title", "description", "icon"]):
                    raise ValueError("Missing required fields in intent")
                    
                intents.append(IntentOption(
                    title=item["title"],
                    description=item["description"],
                    icon=item["icon"]
                ))
                
            return intents
            
        except Exception as e:
            raise Exception(f"Failed to parse Gemini response: {str(e)}")
    
    def _get_default_template(self) -> List[IntentOption]:
        """API ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì˜ë„ í…œí”Œë¦¿ ì œê³µ
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Gemini API ì „ì²´ ì‹¤íŒ¨ ì‹œ ì„œë¹„ìŠ¤ ì—°ì†ì„±ì„ ìœ„í•œ ê¸°ë³¸ ì˜ë„ ì˜µì…˜ ì œê³µ
        - ì¼ë°˜ì ì¸ ì‚¬ìš©ì ëª©í‘œì— ì ìš© ê°€ëŠ¥í•œ 4ê°€ì§€ ë³´í¸ì  ì˜ë„ íƒ€ì…
        - ê° ì˜ë„ëŠ” ì•„ì´ì½˜, ì œëª©, ì„¤ëª…ì„ í¬í•¨í•œ ì™„ì „í•œ êµ¬ì¡°
        - ì‚¬ìš©ìê°€ ë˜‘ê°™ì´ 4ê°€ì§€ ì˜µì…˜ì„ ë°›ì„ ìˆ˜ ìˆë„ë¡ ë³´ì¥
        """
        return [
            IntentOption(
                title="ê³„íš ì„¸ìš°ê¸°",
                description="ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ê³„íšì„ ì„¸ìš°ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                icon="ğŸ“‹"
            ),
            IntentOption(
                title="ì¤€ë¹„í•˜ê¸°",
                description="í•„ìš”í•œ ê²ƒë“¤ì„ ì¤€ë¹„í•˜ê³  ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                icon="âœ…"
            ),
            IntentOption(
                title="ì •ë³´ ì°¾ê¸°",
                description="ê´€ë ¨ëœ ì •ë³´ë¥¼ ì¡°ì‚¬í•˜ê³  ì•Œì•„ë³´ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                icon="ğŸ”"
            ),
            IntentOption(
                title="ë°”ë¡œ ì‹œì‘í•˜ê¸°",
                description="ì§€ê¸ˆ ë‹¹ì¥ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                icon="ğŸš€"
            )
        ]
    
    async def parallel_search(self, queries: List[str]) -> List[SearchResult]:
        """ë‹¤ì¤‘ ê²€ìƒ‰ ì¿¼ë¦¬ ë³‘ë ¬ ì‹¤í–‰
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œë³„ë¡œ ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë³‘ë ¬ ì²˜ë¦¬
        - API ì œí•œ(MAX_CONCURRENT_SEARCHES)ì„ ê³ ë ¤í•œ ë°°ì¹˜ ì²˜ë¦¬
        - ê° ë°°ì¹˜ë³„ë¡œ asyncio.gatherë¡œ ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
        - ì„±ê³µ/ì‹¤íŒ¨ í†µê³„ ë° ë¡œê¹…ìœ¼ë¡œ ê²€ìƒ‰ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
        - ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œì—ë„ ì˜¤ë¥˜ ê²°ê³¼ ê°ì²´ë¡œ ì „ì²´ ê²°ê³¼ì— í¬í•¨
        """
        logger.info("ğŸš€ GEMINI ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘")
        logger.info(f"   ğŸ“ ìš”ì²­ëœ ì¿¼ë¦¬ ìˆ˜: {len(queries)}ê°œ")
        
        if not queries:
            logger.warning("âš ï¸  ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return []
        
        # ì¿¼ë¦¬ ë‚´ìš© ë¡œê¹…
        for i, query in enumerate(queries[:5]):  # ì²˜ìŒ 5ê°œë§Œ ë¡œê¹…
            logger.info(f"   ğŸ” ì¿¼ë¦¬ {i+1}: {query}")
        if len(queries) > 5:
            logger.info(f"   ... ê·¸ ì™¸ {len(queries) - 5}ê°œ ë”")
        
        # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ìˆ˜ì— ë§ê²Œ ëª¨ë“  ì¿¼ë¦¬ ì²˜ë¦¬ (API ì œí•œ ê³ ë ¤)
        max_concurrent_searches = min(len(queries), settings.MAX_CONCURRENT_SEARCHES)
        limited_queries = queries  # ëª¨ë“  ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ë˜ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì‹¤í–‰
        
        if len(queries) > settings.MAX_CONCURRENT_SEARCHES:
            logger.info(f"ğŸ“¦ {len(queries)}ê°œ ì¿¼ë¦¬ë¥¼ {settings.MAX_CONCURRENT_SEARCHES}ê°œì”© ë°°ì¹˜ë¡œ ì²˜ë¦¬")
        else:
            logger.info(f"âœ… {len(queries)}ê°œ ì¿¼ë¦¬ ëª¨ë‘ ë³‘ë ¬ ì²˜ë¦¬")
        
        try:
            logger.info(f"âš¡ {len(limited_queries)}ê°œ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘...")
            
            # ëª¨ë“  ì¿¼ë¦¬ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì²˜ë¦¬
            all_results = []
            batch_size = settings.MAX_CONCURRENT_SEARCHES
            
            for i in range(0, len(limited_queries), batch_size):
                batch_queries = limited_queries[i:i+batch_size]
                logger.info(f"ğŸ”„ ë°°ì¹˜ {i//batch_size + 1}: {len(batch_queries)}ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘...")
                
                # ë°°ì¹˜ë³„ ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
                tasks = [self._search_single_query(query) for query in batch_queries]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                all_results.extend(batch_results)
            
            results = all_results
            
            # ì˜ˆì™¸ ì²˜ë¦¬ ë° ê²°ê³¼ ì •ë¦¬
            processed_results = []
            success_queries = []
            failed_queries = []
            
            for i, result in enumerate(results):
                query = limited_queries[i]
                if isinstance(result, Exception):
                    logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨ [{i+1}]: '{query[:50]}...' - {str(result)}")
                    processed_results.append(self._create_error_result(query, str(result)))
                    failed_queries.append(query)
                else:
                    if result.success:
                        logger.info(f"âœ… ê²€ìƒ‰ ì„±ê³µ [{i+1}]: '{query[:50]}...' ({len(result.content)}ì)")
                        success_queries.append(query)
                    else:
                        logger.warning(f"âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨ [{i+1}]: '{query[:50]}...' - {result.error_message}")
                        failed_queries.append(query)
                    processed_results.append(result)
            
            success_count = len(success_queries)
            failed_count = len(failed_queries)
            
            # ê²°ê³¼ ìš”ì•½
            logger.info("=" * 60)
            logger.info("ğŸ“Š GEMINI ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"âœ… ì„±ê³µ: {success_count}ê°œ")
            logger.info(f"âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
            logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {(success_count/len(limited_queries)*100):.1f}%")
            
            if success_count > 0:
                # ì„±ê³µí•œ ê²€ìƒ‰ ê²°ê³¼ì˜ ë‚´ìš© ê¸¸ì´ í†µê³„
                content_lengths = [len(r.content) for r in processed_results if r.success and r.content]
                if content_lengths:
                    avg_length = sum(content_lengths) / len(content_lengths)
                    min_length = min(content_lengths)
                    max_length = max(content_lengths)
                    logger.info(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: í‰ê·  {avg_length:.0f}ì (ìµœì†Œ {min_length}, ìµœëŒ€ {max_length})")
                
                # ì„±ê³µí•œ ì¿¼ë¦¬ ëª‡ ê°œ ì˜ˆì‹œ
                for query in success_queries[:3]:
                    logger.info(f"   âœ… '{query[:40]}...'")
            
            if failed_count > 0:
                logger.warning(f"âš ï¸  ì‹¤íŒ¨í•œ ì¿¼ë¦¬ {min(3, failed_count)}ê°œ ì˜ˆì‹œ:")
                for query in failed_queries[:3]:
                    logger.warning(f"   âŒ '{query[:40]}...'")
            
            logger.info("=" * 60)
            
            return processed_results
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë³‘ë ¬ ê²€ìƒ‰ ì „ì²´ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"   ğŸ”„ ëª¨ë“  ì¿¼ë¦¬ë¥¼ ì‹¤íŒ¨ ì²˜ë¦¬í•©ë‹ˆë‹¤")
            return [self._create_error_result(query, str(e)) for query in limited_queries]
    
    async def _search_single_query(self, query: str) -> SearchResult:
        """ë‹¨ì¼ ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤ì‹œê°„ ì‹¤í–‰
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ê°œë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì— ëŒ€í•œ Google ê²€ìƒ‰ ê¸°ë°˜ ì‹¤ì‹œê°„ ì •ë³´ ìˆ˜ì§‘
        - get_search_prompt()ë¡œ êµ¬ì¡°í™”ëœ ê²€ìƒ‰ í”„ë¡¬í”„íŠ¸ ìƒì„±
        - Gemini APIì˜ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ìœ¼ë¡œ ìµœì‹  ì •ë³´ íšë“±
        - ê²€ìƒ‰ ì‹œê°„ ì¶”ì  ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        - ì˜ˆì™¸ ë°œìƒ ì‹œ SearchResult ì˜¤ë¥˜ ê°ì²´ë¡œ ì•ˆì „í•œ ì‹¤íŒ¨ ì²˜ë¦¬
        """
        start_time = asyncio.get_event_loop().time()
        logger.debug(f"ğŸ” ë‹¨ì¼ ê²€ìƒ‰ ì‹œì‘: '{query[:50]}...'")
        
        try:
            # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì— ëŒ€í•œ êµ¬ì²´ì ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„± (Structured Output ì‚¬ìš©)
            prompt = get_search_prompt(query)
            logger.debug(f"ğŸ“ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")

            # Gemini API í˜¸ì¶œ (ì›¹ ê²€ìƒ‰ í™œì„±í™”)
            response = await self._call_gemini_api_with_search(prompt)
            elapsed = asyncio.get_event_loop().time() - start_time
            
            # ì‘ë‹µ íŒŒì‹±
            result = self._parse_search_response(query, response)
            
            if result.success:
                logger.debug(f"âœ… ê²€ìƒ‰ ì™„ë£Œ ({elapsed:.2f}ì´ˆ): {len(result.content)}ì ì‘ë‹µ")
            else:
                logger.warning(f"âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ): {result.error_message}")
            
            return result
                        
        except asyncio.TimeoutError:
            elapsed = asyncio.get_event_loop().time() - start_time
            logger.error(f"â° ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ ({elapsed:.2f}ì´ˆ)")
            logger.error(f"   ì¿¼ë¦¬: '{query[:50]}...'")
            return self._create_error_result(query, f"Search timeout after {elapsed:.2f}s")
        except Exception as e:
            elapsed = asyncio.get_event_loop().time() - start_time
            logger.error(f"ğŸ’¥ ê²€ìƒ‰ ì˜ˆì™¸ ë°œìƒ ({elapsed:.2f}ì´ˆ)")
            logger.error(f"   ì¿¼ë¦¬: '{query[:50]}...'")
            logger.error(f"   ì˜¤ë¥˜: {str(e)}")
            return self._create_error_result(query, f"Exception: {str(e)}")
    
    def _parse_search_response(self, query: str, response: str) -> SearchResult:
        """Gemini ì›¹ ê²€ìƒ‰ ì‘ë‹µ êµ¬ì¡°í™” íŒŒì‹±
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Structured Outputìœ¼ë¡œ ìˆ˜ì‹ ëœ JSON í˜•íƒœì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ SearchResult ê°ì²´ë¡œ ë³€í™˜
        - tips, contacts, links, price, location ë“± ë‹¤ì–‘í•œ ì •ë³´ ìœ í˜• ì²˜ë¦¬
        - JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì½˜í…ì¸ ë¥¼ í´ë°± ë°ì´í„°ë¡œ í™œìš©
        - ë§í¬ ì •ë³´ë¥¼ sources ë°°ì—´ë¡œ ì¶”ì¶œí•˜ì—¬ ì†ë§‰ ì¶”ì 
        - ëª¨ë“  ê²½ìš°ì— ìœ íš¨í•œ SearchResult ê°ì²´ ë°˜í™˜ ë³´ì¥
        """
        try:
            if not response or not response.strip():
                return self._create_error_result(query, "Empty response")
            
            content = response.strip()
            logger.debug(f"Parsing structured output response: {content[:200]}...")
            
            # Structured Outputìœ¼ë¡œ ì¸í•´ ì´ë¯¸ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ì–´ì•¼ í•¨
            try:
                structured_data = json.loads(content)
                logger.info(f"Successfully parsed structured JSON response for query: {query[:50]}...")
                
                # ì‘ë‹µ êµ¬ì¡° ê²€ì¦
                if not isinstance(structured_data, dict):
                    logger.warning("Response is not a dictionary, using as-is")
                    structured_data = {"tips": [content], "contacts": [], "links": [], "price": None, "location": None}
                
                # ë§í¬ ì •ë³´ë¥¼ sourcesë¡œ ë³€í™˜
                sources = []
                if "links" in structured_data and isinstance(structured_data["links"], list):
                    for link in structured_data["links"]:
                        if isinstance(link, dict) and "url" in link:
                            sources.append(link["url"])
                        elif isinstance(link, str):
                            sources.append(link)
                
                return SearchResult(
                    query=query,
                    content=json.dumps(structured_data, ensure_ascii=False),
                    sources=sources,
                    success=True
                )
                
            except json.JSONDecodeError as json_err:
                logger.warning(f"Failed to parse structured JSON for query '{query}': {json_err}")
                logger.warning(f"Raw content: {content[:200]}...")
                
                # Structured Output ì‹¤íŒ¨ì‹œ í´ë°±
                fallback_data = {
                    "tips": [content] if content else ["ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."],
                    "contacts": [],
                    "links": [],
                    "price": None,
                    "location": None
                }
                
                return SearchResult(
                    query=query,
                    content=json.dumps(fallback_data, ensure_ascii=False),
                    sources=[],
                    success=True
                )
            
        except Exception as e:
            logger.error(f"Failed to parse Gemini structured response for query '{query}': {str(e)}")
            return self._create_error_result(query, f"Parse error: {str(e)}")
    
    def _create_error_result(self, query: str, error_message: str) -> SearchResult:
        """ê²€ìƒ‰ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ SearchResult ê°ì²´ ìƒì„±
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œì—ë„ ì¼ê´€ëœ SearchResult êµ¬ì¡°ë¡œ ê²°ê³¼ ë°˜í™˜
        - success=Falseë¡œ ì„¤ì •í•˜ì—¬ ìƒìœ„ ë ˆì´ì–´ì—ì„œ ì‹¤íŒ¨ ì²˜ë¦¬ ê°€ëŠ¥
        - error_messageì— ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ ì €ì¥
        - ë¹ˆ contentì™€ sourcesë¡œ ì˜¤ë¥˜ ìƒí™© ëª…ì‹œ
        """
        return SearchResult(
            query=query,
            content="",
            sources=[],
            success=False,
            error_message=error_message
        )
    
    def generate_search_queries_from_checklist(
        self,
        checklist_items: List[str],
        goal: str,
        answers: List[Dict[str, Any]]
    ) -> List[str]:
        """ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ê¸°ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ìƒì„±ëœ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ê°ê°ì„ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ 1:1 ë³€í™˜
        - ì‚¬ìš©ìì˜ ëª©í‘œì™€ ë‹µë³€ ë§¥ë½ì„ ê³ ë ¤í•œ ì¿¼ë¦¬ ìµœì í™”
        - ê° ì•„ì´í…œì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ì œê³µ
        - ëª¨ë“  ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì— ëŒ€í•œ ì¿¼ë¦¬ ë³´ì¥ìœ¼ë¡œ ì™„ì „í•œ ê²€ìƒ‰ ë²”ìœ„
        """
        
        logger.info("ğŸ¯ GEMINI ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì‹œì‘")
        logger.info(f"   ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ: {len(checklist_items)}ê°œ")
        logger.info(f"   ğŸ¯ ëª©í‘œ: {goal[:50]}...")
        logger.info(f"   ğŸ’¬ ë‹µë³€: {len(answers)}ê°œ")
        
        # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ì§ì ‘ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš© (í‚¤ì›Œë“œ ì¶”ì¶œ ì—†ì´)
        search_queries = []
        
        logger.info(f"   ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ì§ì ‘ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš©")
        
        for i, item in enumerate(checklist_items):
            logger.debug(f"   ğŸ” ì•„ì´í…œ {i+1}: '{item[:50]}...'")
            
            # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ê·¸ëŒ€ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš©
            # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ì—ì„œ ì´ ì•„ì´í…œì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ìš”ì²­
            search_queries.append(item)
            logger.debug(f"      â†’ ê²€ìƒ‰ ì¿¼ë¦¬: '{item}'")
        
        logger.info("=" * 60)
        logger.info("ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ â†’ ê²€ìƒ‰ ì¿¼ë¦¬ ë§¤í•‘")
        logger.info("=" * 60)
        for i, item in enumerate(checklist_items):
            logger.info(f"   {i+1:2d}. '{item[:40]}...'")
        logger.info("=" * 60)
        
        logger.info(f"âœ… 1:1 ë§¤í•‘ ì™„ë£Œ: {len(checklist_items)}ê°œ ì•„ì´í…œ â†’ {len(search_queries)}ê°œ ì¿¼ë¦¬")
        
        if not search_queries:
            logger.error("ğŸš¨ ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
            logger.error("   ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì´ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif len(search_queries) != len(checklist_items):
            logger.warning(f"âš ï¸  ì¿¼ë¦¬ ìˆ˜ ë¶ˆì¼ì¹˜: {len(checklist_items)}ê°œ ì•„ì´í…œ vs {len(search_queries)}ê°œ ì¿¼ë¦¬")
        
        return search_queries
    
    
    def _create_gemini_compatible_schema(self) -> Dict[str, Any]:
        """Gemini API í˜¸í™˜ JSON ìŠ¤í‚¤ë§ˆ ìƒì„±
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - SearchResponse Pydantic ëª¨ë¸ì„ Gemini APIì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ JSON ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
        - $defsì™€ $refë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ ì¸ë¼ì¸ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°
        - tips, contacts, links, price, location ë“± ê²€ìƒ‰ ê²°ê³¼ì— í•„ìš”í•œ ëª¨ë“  í•„ë“œ ì •ì˜
        - Structured Outputì„ í†µí•œ ì¼ê´€ë˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì‘ë‹µ í˜•ì‹ ë³´ì¥
        """
        # GeminiëŠ” $defsì™€ $refë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¸ë¼ì¸ ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
        return {
            "type": "object",
            "properties": {
                "tips": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "ì‹¤ìš©ì ì¸ íŒê³¼ ì¡°ì–¸"
                },
                "contacts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {
                                "type": "string",
                                "description": "ì—°ë½ì²˜ ì´ë¦„"
                            },
                            "phone": {
                                "type": "string",
                                "description": "ì „í™”ë²ˆí˜¸"
                            },
                            "email": {
                                "type": "string",
                                "description": "ì´ë©”ì¼ ì£¼ì†Œ"
                            }
                        },
                        "required": ["name"]
                    },
                    "description": "ê´€ë ¨ ì—°ë½ì²˜ ì •ë³´"
                },
                "links": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "title": {
                                "type": "string",
                                "description": "ë§í¬ ì œëª©"
                            },
                            "url": {
                                "type": "string",
                                "description": "ì›¹ì‚¬ì´íŠ¸ URL"
                            }
                        },
                        "required": ["title", "url"]
                    },
                    "description": "ìœ ìš©í•œ ì›¹ì‚¬ì´íŠ¸ ë§í¬"
                },
                "price": {
                    "type": "string",
                    "description": "ì˜ˆìƒ ë¹„ìš© ë˜ëŠ” ê°€ê²© ì •ë³´"
                },
                "location": {
                    "type": "string", 
                    "description": "ìœ„ì¹˜ ë˜ëŠ” ì¥ì†Œ ì •ë³´"
                }
            },
            "required": ["tips", "contacts", "links"]
        }
    
    async def _call_gemini_api_stream_with_validation(self, prompt: str, stream_id: str):
        """Gemini API ê°•í™”ëœ ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì˜ ì‹¤ì‹œê°„ ì™„ì „ì„± ê²€ì¦ í¬í•¨
        - ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ìˆ˜ì‹  ë° ëˆ„ì  ì½˜í…ì¸  ì¶”ì 
        - ì£¼ê¸°ì ì¸ ì§„í–‰ ìƒí™© ë¡œê¹…ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ëª¨ë‹ˆí„°ë§
        - ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ JSON êµ¬ì¡° ì™„ì „ì„± ê²€ì¦
        - ì•ˆì •ì ì¸ ìƒì„± ì½˜í”¼ê·¸ ì„¤ì •ìœ¼ë¡œ ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µ ë³´ì¥
        """
        chunks_received = 0
        total_chars = 0
        accumulated_text = ""
        
        try:
            logger.debug(f"ğŸ” Starting validated streaming request [Stream: {stream_id}] (prompt length: {len(prompt)} chars)")
            
            # Gemini ìŠ¤íŠ¸ë¦¬ë° ì„¤ì • (ë” ì•ˆì •ì ì¸ ì„¤ì •)
            generation_config = genai.types.GenerationConfig(
                max_output_tokens=GeminiConfig.MAX_OUTPUT_TOKENS,
                temperature=GeminiConfig.TEMPERATURE,
                top_p=GeminiConfig.TOP_P,
                top_k=GeminiConfig.TOP_K,
                stop_sequences=None,  # ì¤‘ë‹¨ ì‹œí€€ìŠ¤ ì œê±°
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            response_stream = await asyncio.to_thread(
                self.model.generate_content,
                prompt,
                generation_config=generation_config,
                stream=True
            )
            
            logger.debug(f"âœ… Gemini streaming response initiated [Stream: {stream_id}]")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ (ì™„ì „ì„± ê²€ì¦ í¬í•¨)
            for chunk in response_stream:
                chunk_text = ""
                
                if hasattr(chunk, 'text') and chunk.text:
                    chunk_text = chunk.text
                elif hasattr(chunk, 'candidates') and chunk.candidates:
                    for candidate in chunk.candidates:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    chunk_text += part.text
                
                if chunk_text:
                    chunks_received += 1
                    total_chars += len(chunk_text)
                    accumulated_text += chunk_text
                    
                    # ì£¼ê¸°ì ìœ¼ë¡œ ì§„í–‰ ìƒí™© ë¡œê¹…
                    if chunks_received % 10 == 0:
                        logger.debug(f"ğŸ“Š [Stream: {stream_id}] Chunks: {chunks_received}, Chars: {total_chars}")
                    
                    yield chunk_text
            
            logger.info(f"ğŸ“‹ Stream completed [Stream: {stream_id}]: {chunks_received} chunks, {total_chars} chars")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ JSON êµ¬ì¡° ê²€ì¦
            self._validate_stream_completion(accumulated_text, stream_id, total_chars)
                                    
        except Exception as e:
            logger.error(f"ğŸš¨ Validated streaming API error [Stream: {stream_id}]: {str(e)}")
            logger.debug(f"Error details - Chunks received: {chunks_received}, Total chars: {total_chars}")
            raise Exception(f"Gemini validated streaming failed: {str(e)}")
    
    def _validate_json_completeness(self, content: str, stream_id: str) -> bool:
        """ëˆ„ì ëœ ì½˜í…ì¸ ì˜ JSON ì™„ì „ì„± ê²€ì¦
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ëˆ„ì ëœ ì½˜í…ì¸ ê°€ ì™„ì „í•œ JSON êµ¬ì¡°ì¸ì§€ ê²€ì¦
        - ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ë°ì´í„° ì¶”ì¶œ
        - questions ë°°ì—´ì˜ ì¡´ì¬ ì—¬ë¶€ì™€ ê° ì§ˆë¬¸ì˜ í•„ìˆ˜ í•„ë“œ ê²€ì¦
        - ì§ˆë¬¸ ì˜µì…˜ì˜ ì™„ì „ì„± ë° í…ìŠ¤íŠ¸ ì˜ë¦¼ í˜„ìƒ ê°ì§€
        - ê²€ì¦ ì‹¤íŒ¨ ì‹œ ìƒì„¸ ë¡œê¹…ìœ¼ë¡œ ë””ë²„ê¹… ì •ë³´ ì œê³µ
        """
        try:
            # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì œê±°
            clean_content = self._extract_json_from_markdown(content)
            
            # JSON íŒŒì‹± ì‹œë„
            parsed = json.loads(clean_content)
            
            # ì§ˆë¬¸ êµ¬ì¡° ê²€ì¦
            if not isinstance(parsed, dict) or 'questions' not in parsed:
                logger.warning(f"ğŸš¨ Invalid JSON structure [Stream: {stream_id}]: missing 'questions' field")
                return False
            
            questions = parsed['questions']
            if not isinstance(questions, list) or len(questions) == 0:
                logger.warning(f"ğŸš¨ Invalid questions array [Stream: {stream_id}]: empty or not a list")
                return False
            
            # ê° ì§ˆë¬¸ ê²€ì¦
            for i, question in enumerate(questions):
                if not isinstance(question, dict):
                    logger.warning(f"ğŸš¨ Question {i} is not a dict [Stream: {stream_id}]")
                    return False
                
                required_fields = ['id', 'text', 'type', 'options']
                for field in required_fields:
                    if field not in question:
                        logger.warning(f"ğŸš¨ Question {i} missing field '{field}' [Stream: {stream_id}]")
                        return False
                
                # ì˜µì…˜ ê²€ì¦ (multiple typeì¸ ê²½ìš°)
                if question['type'] == 'multiple':
                    options = question['options']
                    if not isinstance(options, list) or len(options) == 0:
                        logger.warning(f"ğŸš¨ Question {i} has invalid options [Stream: {stream_id}]")
                        return False
                    
                    # ê° ì˜µì…˜ ê²€ì¦
                    for j, option in enumerate(options):
                        if isinstance(option, dict):
                            if 'id' not in option or 'text' not in option:
                                logger.warning(f"ğŸš¨ Question {i}, Option {j} missing required fields [Stream: {stream_id}]")
                                return False
            
            logger.info(f"âœ… JSON validation passed [Stream: {stream_id}]: {len(questions)} questions validated")
            return True
            
        except json.JSONDecodeError as e:
            logger.warning(f"ğŸš¨ JSON parsing failed [Stream: {stream_id}]: {str(e)}")
            logger.debug(f"Content preview: {content[:500]}...")
            return False
        except Exception as e:
            logger.error(f"ğŸš¨ JSON validation error [Stream: {stream_id}]: {str(e)}")
            return False
    
    def _validate_stream_completion(self, content: str, stream_id: str, total_chars: int):
        """ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì½˜í…ì¸  ë¬´ê²°ì„± ê²€ì¦
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ì „ì²´ ìŠ¤íŠ¸ë¦¬ë°ì´ ì™„ë£Œëœ í›„ ì½˜í…ì¸ ì˜ ì™„ì „ì„± ìµœì¢… ì ê²€
        - ì½˜í…ì¸  ê¸¸ì´ê°€ ì˜ˆìƒë³´ë‹¤ ë„ˆë¬´ ì§§ì€ì§€ í™•ì¸
        - JSON êµ¬ì¡°ì˜ ë°”ëŒë§ê´„í˜¸ì™€ ëŒ€ê´„í˜¸ ê· í˜• ê²€ì¦
        - ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì´ ì™„ì „íˆ ë‹«í˜€ìˆëŠ”ì§€ í™•ì¸
        - ê° ê²€ì¦ ë‹¨ê³„ë³„ ìƒì„¸ ê²½ê³  ë¡œê¹…
        """
        try:
            # ê¸°ë³¸ ê¸¸ì´ ê²€ì¦ (ë„ˆë¬´ ì§§ìœ¼ë©´ ë¶ˆì™„ì „)
            if total_chars < GeminiConfig.MIN_CONTENT_LENGTH:
                logger.warning(f"ğŸš¨ Stream suspiciously short [Stream: {stream_id}]: {total_chars} chars")
            
            # JSON êµ¬ì¡° ì™„ë£Œ ê²€ì¦
            brace_count = content.count('{') - content.count('}')
            bracket_count = content.count('[') - content.count(']')
            
            if brace_count != 0 or bracket_count != 0:
                logger.warning(f"ğŸš¨ Unbalanced brackets detected [Stream: {stream_id}]: braces={brace_count}, brackets={bracket_count}")
            
            # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì™„ë£Œ ê²€ì¦
            if '```json' in content and not content.rstrip().endswith('```'):
                logger.warning(f"ğŸš¨ Incomplete markdown block [Stream: {stream_id}]")
                
        except Exception as e:
            logger.error(f"ğŸš¨ Stream completion validation error [Stream: {stream_id}]: {str(e)}")
    
    def _extract_json_from_markdown(self, content: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ ìˆœìˆ˜ JSON ì¶”ì¶œ
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Gemini APIì—ì„œ ë°˜í™˜í•˜ëŠ” ```json...``` í˜•íƒœì˜ ë§ˆí¬ë‹¤ìš´ ë˜í•‘ ì œê±°
        - JSON ì½”ë“œ ë¸”ë¡ì´ ìˆìœ¼ë©´ ë‚´ë¶€ JSONë§Œ ì¶”ì¶œ
        - ì½”ë“œ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì²¨ë²ˆì§¸ {ë¶€í„° ë§ˆì§€ë§‰ }ê¹Œì§€ ì¶”ì¶œ
        - ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì½˜í…ì¸  ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì—¬ ì•ˆì •ì„± ë³´ì¥
        """
        try:
            content = content.strip()
            
            # ```json...``` íŒ¨í„´ ì°¾ê¸°
            if '```json' in content:
                start = content.find('```json') + 7
                end = content.rfind('```')
                if end > start:
                    json_content = content[start:end].strip()
                    return json_content
            
            # JSON íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì „ì²´ ë‚´ìš© ë°˜í™˜ (ì²« { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€)
            first_brace = content.find('{')
            last_brace = content.rfind('}')
            
            if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
                return content[first_brace:last_brace + 1]
            
            return content
            
        except Exception as e:
            logger.error(f"JSON extraction error: {str(e)}")
            return content
    
    async def _generate_fallback_questions(self, goal: str, intent_title: str, user_country: Optional[str], user_language: Optional[str], country_option: bool) -> Optional[str]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ë¹„ìŠ¤íŠ¸ë¦¬ë° í´ë°± ì§ˆë¬¸ ìƒì„±
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ìŠ¤íŠ¸ë¦¬ë° JSONì´ ë¶ˆì™„ì „í•  ë•Œ ì¼ë°˜ APIë¡œ ì™„ì „í•œ ì§ˆë¬¸ ë‹¤ì‹œ ìƒì„±
        - ë™ì¼í•œ ë§¤ê°œë³€ìˆ˜(goal, intent, country, language)ë¡œ ì¼ê´€ì„± ìœ ì§€
        - generate_questions() ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ì¸ ë¹„ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
        - Question ê°ì²´ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ëŒ€ì²´ ë°ì´í„° ì œê³µ
        - í´ë°± ì‹¤íŒ¨ ì‹œ None ë°˜í™˜ìœ¼ë¡œ ìƒìœ„ ë˜ì´ì–´ì— ì˜¤ë¥˜ ì „íŒŒ
        """
        try:
            logger.info(f"ğŸ”„ Generating fallback questions for: {goal} (intent: {intent_title})")
            
            # ì¼ë°˜ API í˜¸ì¶œë¡œ ì™„ì „í•œ ì§ˆë¬¸ ìƒì„±
            questions = await self.generate_questions(goal, intent_title, user_country, user_language, country_option)
            
            if questions:
                fallback_json = json.dumps({"questions": [q.dict() for q in questions]}, ensure_ascii=False, indent=2)
                logger.info(f"âœ… Fallback questions generated: {len(questions)} questions, {len(fallback_json)} chars")
                return fallback_json
            
        except Exception as e:
            logger.error(f"ğŸš¨ Fallback question generation failed: {str(e)}")
        
        return None

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
gemini_service = GeminiService() 
