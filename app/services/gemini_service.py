import json
import asyncio
from typing import List, Dict, Optional
import google.generativeai as genai
from app.core.config import settings
from app.schemas.nowwhat import IntentOption
from app.schemas.questions import Question, Option
from app.prompts.prompt_selector import (
    get_intent_analysis_prompt, get_questions_generation_prompt, get_checklist_generation_prompt,
    get_intent_analysis_response_class, get_questions_list_response_class, get_checklist_response_class
)
from app.prompts.search_prompts import get_search_prompt, SearchResponse
from app.prompts.enhanced_prompts import get_enhanced_knowledge_prompt
import logging
import uuid
from dataclasses import dataclass
from typing import Any
from pydantic import BaseModel

logger = logging.getLogger(__name__)
# Gemini ì„œë¹„ìŠ¤ ë””ë²„ê¹…ì„ ìœ„í•´ ì„ì‹œë¡œ DEBUG ë ˆë²¨ ì„¤ì •
logger.setLevel(logging.DEBUG)

# Pydantic ëª¨ë¸ë“¤ì€ ì´ì œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì—ì„œ import

@dataclass
class SearchResult:
    """Gemini API ê²€ìƒ‰ ê²°ê³¼"""
    query: str
    content: str
    sources: List[str]
    success: bool
    error_message: Optional[str] = None
# ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€ (ì´ë¯¸ ìˆë‹¤ë©´ ë¬´ì‹œë¨)
if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

# Gemini API ì„¤ì •
if settings.GEMINI_API_KEY:
    genai.configure(api_key=settings.GEMINI_API_KEY)
    logger.info(f"Gemini API configured with key: {settings.GEMINI_API_KEY[:10]}...{settings.GEMINI_API_KEY[-4:]}")
else:
    logger.error("GEMINI_API_KEY not found in settings")

class GeminiService:
    def __init__(self):
        if not settings.GEMINI_API_KEY:
            logger.error("Cannot initialize GeminiService: GEMINI_API_KEY not set")
            raise ValueError("GEMINI_API_KEY not configured")
        
        self.model = genai.GenerativeModel(settings.GEMINI_MODEL)
        logger.info(f"GeminiService initialized with model: {settings.GEMINI_MODEL}")
        
    async def analyze_intent(self, goal: str, country_info: str = "", language_info: str = "", country_option: bool = True) -> List[IntentOption]:
        """ì‚¬ìš©ì ëª©í‘œë¥¼ ë¶„ì„í•˜ì—¬ 4ê°€ì§€ ì˜ë„ ì˜µì…˜ ìƒì„±"""
        try:
            # language_infoì—ì„œ ì‚¬ìš©ì ì–¸ì–´ ì¶”ì¶œ
            user_language = self._extract_user_language(language_info)
            prompt = self._create_prompt(goal, country_info, language_info, user_language, country_option)
            
            # 3íšŒ ì¬ì‹œë„ ë¡œì§
            for attempt in range(3):
                try:
                    response = await self._call_gemini_api(prompt)
                    intents = self._parse_response(response)
                    
                    if len(intents) == 4:
                        return intents
                    else:
                        logger.warning(f"Gemini returned {len(intents)} intents instead of 4 (attempt {attempt + 1})")
                        
                except Exception as e:
                    logger.error(f"Gemini API call failed (attempt {attempt + 1}): {str(e)}")
                    if attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì§€ì—° í›„ ì¬ì‹œë„
                        await asyncio.sleep(2 ** attempt)  # exponential backoff
                    
            # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í…œí”Œë¦¿ ë°˜í™˜
            logger.warning("All Gemini API attempts failed, using default template")
            return self._get_default_template()
            
        except Exception as e:
            logger.error(f"Intent analysis failed: {str(e)}")
            return self._get_default_template()

    async def generate_questions(
        self, 
        goal: str, 
        intent_title: str, 
        user_country: Optional[str] = None,
        user_language: Optional[str] = None,
        country_option: bool = True
    ) -> List[Question]:
        """ì„ íƒëœ ì˜ë„ì— ë”°ë¥¸ ë§ì¶¤ ì§ˆë¬¸ ìƒì„±"""
        try:
            prompt = self._create_questions_prompt(goal, intent_title, user_country, user_language, country_option)
            
            # 3íšŒ ì¬ì‹œë„ ë¡œì§ (API í˜¸ì¶œ ì‹¤íŒ¨ì‹œì—ë§Œ)
            for attempt in range(3):
                try:
                    response = await self._call_gemini_api(prompt)
                    questions = self._parse_questions_response(response)
                    
                    # ì§ˆë¬¸ ê°œìˆ˜ì— ê´€ê³„ì—†ì´ ë°˜í™˜
                    logger.info(f"Gemini returned {len(questions)} questions")
                    return questions
                        
                except Exception as e:
                    logger.error(f"Gemini questions API call failed (attempt {attempt + 1}): {str(e)}")
                    if attempt < 2:
                        await asyncio.sleep(2 ** attempt)
                    
            # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ì‹œ ìºì‹œëœ ì§ˆë¬¸ í…œí”Œë¦¿ ë°˜í™˜
            logger.warning("All Gemini questions API attempts failed, using cached template")
            return self._get_cached_questions_template(intent_title)
            
        except Exception as e:
            logger.error(f"Question generation failed: {str(e)}")
            return self._get_cached_questions_template(intent_title)

    async def generate_questions_stream(
        self, 
        goal: str, 
        intent_title: str, 
        user_country: Optional[str] = None,
        user_language: Optional[str] = None,
        country_option: bool = True
    ):
        """ì„ íƒëœ ì˜ë„ì— ë”°ë¥¸ ë§ì¶¤ ì§ˆë¬¸ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)"""
        try:
            prompt = self._create_questions_prompt(goal, intent_title, user_country, user_language, country_option)
            
            logger.info(f"ğŸŒŠ Starting streaming question generation for: {goal} (intent: {intent_title})")
            
            # Gemini ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
            async for chunk in self._call_gemini_api_stream(prompt):
                yield chunk
                
        except Exception as e:
            logger.error(f"Streaming question generation failed: {str(e)}")
            # ì—ëŸ¬ ë°œìƒì‹œ ìºì‹œëœ í…œí”Œë¦¿ì„ ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœë¡œ ë°˜í™˜
            cached_questions = self._get_cached_questions_template(intent_title)
            
            # JSON í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë°
            import json
            questions_json = json.dumps([q.dict() for q in cached_questions], ensure_ascii=False)
            
            # ë¬¸ìë³„ë¡œ ì²œì²œíˆ ì „ì†¡ (ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼)
            for i, char in enumerate(questions_json):
                if i % 10 == 0:  # 10ê¸€ìë§ˆë‹¤ ì•½ê°„ì˜ ì§€ì—°
                    await asyncio.sleep(0.01)
                yield char

    def _create_questions_prompt(self, goal: str, intent_title: str, user_country: Optional[str] = None, user_language: Optional[str] = None, country_option: bool = True) -> str:
        """ì§ˆë¬¸ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        country_context = self._get_country_context(user_country)
        language_context = self._get_language_context(user_language)
        
        return get_questions_generation_prompt(
            goal=goal,
            intent_title=intent_title,
            user_country=user_country or "ì •ë³´ ì—†ìŒ",
            user_language=user_language or "ì •ë³´ ì—†ìŒ",
            country_context=country_context,
            language_context=language_context,
            country_option=country_option
        )

    def _get_country_context(self, user_country: Optional[str]) -> str:
        """êµ­ê°€ë³„ ë§ì¶¤ ì»¨í…ìŠ¤íŠ¸"""
        contexts = {
            "KR": "í•œêµ­ ê±°ì£¼ì ê¸°ì¤€, í•œêµ­ ë¬¸í™”ì™€ í™˜ê²½ ê³ ë ¤",
            "US": "ë¯¸êµ­ ê±°ì£¼ì ê¸°ì¤€, ë¯¸êµ­ ë¬¸í™”ì™€ í™˜ê²½ ê³ ë ¤", 
            "JP": "ì¼ë³¸ ê±°ì£¼ì ê¸°ì¤€, ì¼ë³¸ ë¬¸í™”ì™€ í™˜ê²½ ê³ ë ¤",
            "CN": "ì¤‘êµ­ ê±°ì£¼ì ê¸°ì¤€, ì¤‘êµ­ ë¬¸í™”ì™€ í™˜ê²½ ê³ ë ¤"
        }
        return contexts.get(user_country, "ê¸€ë¡œë²Œ ê¸°ì¤€")

    def _get_language_context(self, user_language: Optional[str]) -> str:
        """ì–¸ì–´ë³„ ë§ì¶¤ ì»¨í…ìŠ¤íŠ¸"""
        contexts = {
            "ko": "í•œêµ­ì–´ ê¸°ì¤€, í•œêµ­ ë¬¸í™”ì  ë§¥ë½ ê³ ë ¤",
            "en": "English, Western cultural context",
            "ja": "æ—¥æœ¬èªã€æ—¥æœ¬ã®æ–‡åŒ–çš„æ–‡è„ˆã‚’è€ƒæ…®",
            "zh": "ä¸­æ–‡ï¼Œä¸­å›½æ–‡åŒ–èƒŒæ™¯è€ƒè™‘",
            "es": "EspaÃ±ol, contexto cultural hispano",
            "fr": "FranÃ§ais, contexte culturel franÃ§ais"
        }
        return contexts.get(user_language, "ë‹¤êµ­ì–´ ì§€ì›")

    def _parse_questions_response(self, response: str) -> List[Question]:
        """Gemini ì§ˆë¬¸ ì‘ë‹µ íŒŒì‹±"""
        try:
            # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if not response or not response.strip():
                logger.warning("Gemini returned empty response")
                raise ValueError("Empty response from Gemini")
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            response = response.strip()
            logger.debug(f"Raw Gemini response: {response[:200]}...")  # ë””ë²„ê¹…ìš©
            
            if response.startswith("```json"):
                response = response[7:]
            if response.endswith("```"):
                response = response[:-3]
            response = response.strip()
            
            # ë¹ˆ ì‘ë‹µ ì¬í™•ì¸
            if not response:
                logger.warning("Response became empty after cleaning")
                raise ValueError("Empty response after cleaning")
            
            questions_data = json.loads(response)
            
            if not isinstance(questions_data, list):
                raise ValueError("Response is not a list")
                
            questions = []
            for item in questions_data:
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                if not all(key in item for key in ["id", "text", "type", "required"]):
                    raise ValueError("Missing required fields in question")
                
                # options ì²˜ë¦¬
                options = None
                if item["type"] == "multiple" and "options" in item:
                    options = [
                        Option(
                            id=opt["id"],
                            text=opt["text"], 
                            value=opt["value"]
                        ) for opt in item["options"]
                    ]
                
                questions.append(Question(
                    id=item["id"],
                    text=item["text"],
                    type=item["type"],
                    options=options,
                    required=item["required"]
                ))
                
            return questions
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {str(e)}, Response: '{response[:500]}'")
            raise Exception(f"Failed to parse Gemini questions response: Invalid JSON - {str(e)}")
        except Exception as e:
            logger.error(f"Questions parsing error: {str(e)}, Response: '{response[:500] if 'response' in locals() else 'N/A'}'")
            raise Exception(f"Failed to parse Gemini questions response: {str(e)}")

    def _get_cached_questions_template(self, intent_title: str) -> List[Question]:
        """ì˜ë„ë³„ ìºì‹œëœ ì§ˆë¬¸ í…œí”Œë¦¿"""
        templates = {
            "ì—¬í–‰ ê³„íš": [
                Question(
                    id="q_duration",
                    text="ì—¬í–‰ ê¸°ê°„ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?",
                    type="multiple",
                    options=[
                        Option(id="opt_3days", text="2ë°• 3ì¼", value="3days"),
                        Option(id="opt_5days", text="4ë°• 5ì¼", value="5days"),
                        Option(id="opt_1week", text="1ì£¼ì¼", value="1week"),
                        Option(id="opt_longer", text="1ì£¼ì¼ ì´ìƒ", value="longer")
                    ],
                    required=True
                ),
                Question(
                    id="q_companions",
                    text="ëˆ„êµ¬ì™€ í•¨ê»˜ ê°€ì‹œë‚˜ìš”?",
                    type="multiple",
                    options=[
                        Option(id="opt_alone", text="í˜¼ì", value="alone"),
                        Option(id="opt_couple", text="ì—°ì¸/ë°°ìš°ìì™€", value="couple"),
                        Option(id="opt_family", text="ê°€ì¡±ê³¼", value="family"),
                        Option(id="opt_friends", text="ì¹œêµ¬ë“¤ê³¼", value="friends")
                    ],
                    required=True
                ),
                Question(
                    id="q_activities",
                    text="ì£¼ë¡œ í•˜ê³  ì‹¶ì€ í™œë™ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    type="multiple",
                    options=[
                        Option(id="opt_sightseeing", text="ê´€ê´‘/ëª…ì†Œ íƒë°©", value="sightseeing"),
                        Option(id="opt_food", text="ë§›ì§‘ íƒë°©", value="food"),
                        Option(id="opt_shopping", text="ì‡¼í•‘", value="shopping"),
                        Option(id="opt_culture", text="ë¬¸í™” ì²´í—˜", value="culture")
                    ],
                    required=True
                )
            ],
            "ê³„íš ì„¸ìš°ê¸°": [
                Question(
                    id="q_timeline",
                    text="ì–¸ì œê¹Œì§€ ê³„íšì„ ì™„ì„±í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                    type="multiple",
                    options=[
                        Option(id="opt_1week", text="1ì£¼ì¼ ë‚´", value="1week"),
                        Option(id="opt_1month", text="1ë‹¬ ë‚´", value="1month"),
                        Option(id="opt_3months", text="3ë‹¬ ë‚´", value="3months"),
                        Option(id="opt_flexible", text="ìœ ì—°í•˜ê²Œ", value="flexible")
                    ],
                    required=True
                ),
                Question(
                    id="q_priority",
                    text="ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê²ƒì€?",
                    type="multiple",
                    options=[
                        Option(id="opt_time", text="ì‹œê°„ íš¨ìœ¨ì„±", value="time"),
                        Option(id="opt_cost", text="ë¹„ìš© ì ˆì•½", value="cost"),
                        Option(id="opt_quality", text="í’ˆì§ˆ/ë§Œì¡±ë„", value="quality"),
                        Option(id="opt_convenience", text="í¸ì˜ì„±", value="convenience")
                    ],
                    required=True
                )
            ]
        }
        
        return templates.get(intent_title, self._get_default_questions_template())

    def _get_default_questions_template(self) -> List[Question]:
        """ê¸°ë³¸ ì§ˆë¬¸ í…œí”Œë¦¿"""
        return [
            Question(
                id="q_when",
                text="ì–¸ì œê¹Œì§€ ì´ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                type="multiple",
                options=[
                    Option(id="opt_1week", text="1ì£¼ì¼ ë‚´", value="1week"),
                    Option(id="opt_1month", text="1ë‹¬ ë‚´", value="1month"),
                    Option(id="opt_3months", text="3ë‹¬ ë‚´", value="3months"),
                    Option(id="opt_6months", text="6ë‹¬ ë‚´", value="6months")
                ],
                required=True
            ),
            Question(
                id="q_resources",
                text="ì´ ëª©í‘œë¥¼ ìœ„í•´ íˆ¬ìí•  ìˆ˜ ìˆëŠ” ìì›ì€?",
                type="multiple", 
                options=[
                    Option(id="opt_time", text="ì£¼ë¡œ ì‹œê°„", value="time"),
                    Option(id="opt_money", text="ì£¼ë¡œ ëˆ", value="money"),
                    Option(id="opt_both", text="ì‹œê°„ê³¼ ëˆ ëª¨ë‘", value="both"),
                    Option(id="opt_minimal", text="ìµœì†Œí•œë§Œ", value="minimal")
                ],
                required=True
            ),
            Question(
                id="q_priority",
                text="ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê²ƒì€?",
                type="multiple",
                options=[
                    Option(id="opt_speed", text="ë¹ ë¥¸ ë‹¬ì„±", value="speed"),
                    Option(id="opt_quality", text="ë†’ì€ í’ˆì§ˆ", value="quality"),
                    Option(id="opt_cost", text="ë¹„ìš© ì ˆì•½", value="cost"),
                    Option(id="opt_balance", text="ê· í˜•ì¡íŒ ì ‘ê·¼", value="balance")
                ],
                required=True
            )
        ]
    
    def _create_prompt(self, goal: str, country_info: str = "", language_info: str = "", user_language: str = None, country_option: bool = True) -> str:
        """Gemini APIìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return get_intent_analysis_prompt(goal, country_info, language_info, user_language, country_option)
    
    def _extract_user_language(self, language_info: str) -> str:
        """language_info ë¬¸ìì—´ì—ì„œ ì‚¬ìš©ì ì–¸ì–´ ì¶”ì¶œ"""
        if not language_info:
            return None
        
        # "ì‚¬ìš©ì ì–¸ì–´: ko" í˜•íƒœì—ì„œ ì–¸ì–´ ì½”ë“œ ì¶”ì¶œ
        if ":" in language_info:
            return language_info.split(":")[-1].strip()
        
        return language_info.strip()

    async def _call_gemini_api_with_search(self, prompt: str) -> str:
        """Gemini API í˜¸ì¶œ (ê³µì‹ Google Search ê¸°ëŠ¥ ì‚¬ìš©)"""
        try:
            logger.debug(f"Sending search prompt to Gemini (length: {len(prompt)} chars)")
            
            # ê³µì‹ Google Search grounding êµ¬í˜„ + Structured Output
            try:
                # Google Search ë„êµ¬ ì„¤ì • (ê³µì‹ ë°©ë²•)
                search_tool = genai.protos.Tool(
                    google_search_retrieval=genai.protos.GoogleSearchRetrieval()
                )
                
                logger.debug("Using Google Search grounding tool with structured output")
                
                # SearchResponseë¥¼ Gemini í˜¸í™˜ ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
                response_schema = self._create_gemini_compatible_schema()
                
                response = await asyncio.to_thread(
                    self.model.generate_content,
                    prompt,
                    tools=[search_tool],
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=8192,
                        temperature=0.7,
                        top_p=0.8,
                        top_k=40,
                        response_mime_type="application/json",
                        response_schema=response_schema
                    )
                )
                
                logger.debug("Google Search grounding with structured output completed")
                
            except Exception as tool_error:
                logger.warning(f"Google Search grounding failed: {tool_error}")
                logger.debug(f"Error type: {type(tool_error).__name__}")
                logger.info("Trying alternative Google Search implementation")
                
                try:
                    # ëŒ€ì•ˆì  êµ¬í˜„ (ìµœì‹  SDK) + Structured Output
                    from google.generativeai.types import Tool
                    
                    # ìµœì‹  SDKì˜ GoogleSearch ë„êµ¬
                    search_tool = Tool(
                        google_search_retrieval={}
                    )
                    
                    # Gemini í˜¸í™˜ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
                    response_schema = self._create_gemini_compatible_schema()
                    
                    response = await asyncio.to_thread(
                        self.model.generate_content,
                        prompt,
                        tools=[search_tool],
                        generation_config=genai.types.GenerationConfig(
                            max_output_tokens=16384,  # ê²€ìƒ‰ ê¸°ëŠ¥ í† í° ì œí•œ ì¦ê°€
                            temperature=0.7,
                            top_p=0.8,
                            top_k=40,
                            response_mime_type="application/json",
                            response_schema=response_schema
                        )
                    )
                    
                except Exception as alt_error:
                    logger.warning(f"Alternative Google Search failed: {alt_error}")
                    logger.debug(f"Alternative error type: {type(alt_error).__name__}")
                    logger.info("Using enhanced knowledge-based prompt")
                    
                    # ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, ìµœì‹  ì •ë³´ ìš”ì²­ í”„ë¡¬í”„íŠ¸ + Structured Output
                    enhanced_prompt = get_enhanced_knowledge_prompt(prompt)

                    # Gemini í˜¸í™˜ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
                    response_schema = self._create_gemini_compatible_schema()
                    
                    response = await asyncio.to_thread(
                        self.model.generate_content,
                        enhanced_prompt,
                        generation_config=genai.types.GenerationConfig(
                            max_output_tokens=16384,  # ê²€ìƒ‰ ê¸°ëŠ¥ í† í° ì œí•œ ì¦ê°€
                            temperature=0.7,
                            top_p=0.8,
                            top_k=40,
                            response_mime_type="application/json",
                            response_schema=response_schema
                        )
                    )
            
            # ì‘ë‹µ ì²˜ë¦¬
            if not response:
                logger.error("Gemini returned None response")
                raise Exception("Gemini returned None response")
            
            # grounding metadata í™•ì¸ (ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€)
            if hasattr(response, 'candidates') and response.candidates:
                for candidate in response.candidates:
                    if hasattr(candidate, 'grounding_metadata'):
                        logger.info("Response includes grounding metadata (web search results)")
                        if hasattr(candidate.grounding_metadata, 'search_entry_point'):
                            logger.debug(f"Search entry point: {candidate.grounding_metadata.search_entry_point}")
                        if hasattr(candidate.grounding_metadata, 'grounding_chunks'):
                            logger.debug(f"Found {len(candidate.grounding_metadata.grounding_chunks)} grounding chunks")
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if not hasattr(response, 'text'):
                logger.debug("Extracting text from candidates...")
                if hasattr(response, 'candidates') and response.candidates:
                    for candidate in response.candidates:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    response_text = part.text
                                    break
                    else:
                        raise Exception("No text found in candidates")
                else:
                    raise Exception("No candidates in response")
            else:
                response_text = response.text
            
            logger.debug(f"Gemini search response received (length: {len(response_text) if response_text else 0})")
            
            if not response_text or not response_text.strip():
                logger.error("Gemini returned empty response")
                raise Exception("Gemini returned empty response")
                
            return response_text
            
        except Exception as e:
            logger.error(f"Gemini search API call error: {str(e)}")
            logger.debug(f"Final error type: {type(e).__name__}")
            # ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨ì‹œ ì¼ë°˜ APIë¡œ í´ë°±
            logger.info("Falling back to regular Gemini API without search")
            return await self._call_gemini_api(prompt)
    
    async def _call_gemini_api(self, prompt: str) -> str:
        """Gemini API í˜¸ì¶œ"""
        try:
            logger.debug(f"Sending prompt to Gemini (length: {len(prompt)} chars)")
            response = await asyncio.to_thread(
                self.model.generate_content, 
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=16384,  # ì§ˆë¬¸ ìƒì„± í† í° ì œí•œ ì¦ê°€
                    temperature=0.7,
                    top_p=0.8,
                    top_k=40
                )
            )
            
            # ì‘ë‹µ ìƒíƒœ í™•ì¸
            if not response:
                logger.error("Gemini returned None response")
                raise Exception("Gemini returned None response")
            
            # ì‘ë‹µ ê°ì²´ êµ¬ì¡° í™•ì¸
            logger.debug(f"Gemini response type: {type(response)}")
            logger.debug(f"Gemini response attributes: {dir(response)}")
            
            # Safety rating ë° finish reason í™•ì¸
            if hasattr(response, 'candidates') and response.candidates:
                for i, candidate in enumerate(response.candidates):
                    finish_reason = candidate.finish_reason if hasattr(candidate, 'finish_reason') else 'N/A'
                    logger.debug(f"Candidate {i} finish_reason: {finish_reason}")
                    
                    # finish_reason í•´ì„
                    if finish_reason == 2:
                        logger.warning("Response was truncated due to MAX_TOKENS limit - consider increasing max_output_tokens")
                    elif finish_reason == 3:
                        logger.warning("Response was blocked by safety filters")
                    elif finish_reason == 4:
                        logger.warning("Response was blocked due to recitation concerns")
                    
                    if hasattr(candidate, 'safety_ratings'):
                        logger.debug(f"Candidate {i} safety_ratings: {candidate.safety_ratings}")
            
            if not hasattr(response, 'text'):
                logger.error(f"Gemini response has no text attribute: {type(response)}")
                # ëŒ€ì•ˆìœ¼ë¡œ candidatesì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                if hasattr(response, 'candidates') and response.candidates:
                    for candidate in response.candidates:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    logger.debug(f"Found text in candidate.content.parts: {part.text[:100]}...")
                                    response_text = part.text
                                    break
                    else:
                        raise Exception("Gemini response has no text attribute and no text in candidates")
                else:
                    raise Exception("Gemini response has no text attribute")
            else:
                response_text = response.text
            
            logger.debug(f"Raw Gemini response (length: {len(response_text) if response_text else 0}): '{response_text}'")
            
            if not response_text or not response_text.strip():
                logger.error("Gemini returned empty or whitespace-only response")
                raise Exception("Gemini returned empty response")
                
            return response_text
            
        except Exception as e:
            logger.error(f"Gemini API call error details: {str(e)}")
            raise Exception(f"Gemini API call failed: {str(e)}")
    
    async def _call_gemini_api_stream(self, prompt: str):
        """Gemini API ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ"""
        try:
            logger.debug(f"Starting streaming request to Gemini (prompt length: {len(prompt)} chars)")
            
            # Gemini ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •
            generation_config = genai.types.GenerationConfig(
                max_output_tokens=16384,  # ì§ˆë¬¸ ìƒì„± í† í° ì œí•œ ì¦ê°€
                temperature=0.7,
                top_p=0.8,
                top_k=40
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            response_stream = await asyncio.to_thread(
                self.model.generate_content,
                prompt,
                generation_config=generation_config,
                stream=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )
            
            logger.debug("Gemini streaming response initiated")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            for chunk in response_stream:
                if hasattr(chunk, 'text') and chunk.text:
                    yield chunk.text
                elif hasattr(chunk, 'candidates') and chunk.candidates:
                    for candidate in chunk.candidates:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    yield part.text
                                    
        except Exception as e:
            logger.error(f"Gemini streaming API error: {str(e)}")
            logger.debug(f"Error type: {type(e).__name__}")
            raise Exception(f"Gemini streaming API failed: {str(e)}")
    
    def _parse_response(self, response: str) -> List[IntentOption]:
        """Gemini ì‘ë‹µ íŒŒì‹±"""
        try:
            logger.debug(f"Intent parsing - Raw response: '{response[:200]}...' (total length: {len(response)})")
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
            response = response.strip()
            logger.debug(f"Intent parsing - After strip: '{response[:200]}...'")
            
            if response.startswith("```json"):
                response = response[7:]
                logger.debug("Intent parsing - Removed ```json prefix")
            if response.endswith("```"):
                response = response[:-3]
                logger.debug("Intent parsing - Removed ``` suffix")
            response = response.strip()
            
            logger.debug(f"Intent parsing - Final JSON to parse: '{response[:200]}...'")
            
            if not response:
                logger.error("Intent parsing - Response became empty after cleaning")
                raise ValueError("Empty response after cleaning")
            
            json_data = json.loads(response)
            logger.debug(f"Intent parsing - Successfully parsed JSON: {type(json_data)}")
            
            # JSON ì‘ë‹µì´ {"intents": [...]} í˜•íƒœì¸ì§€ í™•ì¸
            if isinstance(json_data, dict) and "intents" in json_data:
                intents_data = json_data["intents"]
                logger.debug(f"Intent parsing - Found intents array with {len(intents_data)} items")
            elif isinstance(json_data, list):
                intents_data = json_data
                logger.debug(f"Intent parsing - Using direct list with {len(intents_data)} items")
            else:
                raise ValueError(f"Unexpected response format: {type(json_data)}")
                
            intents = []
            for item in intents_data:
                if not all(key in item for key in ["title", "description", "icon"]):
                    raise ValueError("Missing required fields in intent")
                    
                intents.append(IntentOption(
                    title=item["title"],
                    description=item["description"],
                    icon=item["icon"]
                ))
                
            return intents
            
        except Exception as e:
            raise Exception(f"Failed to parse Gemini response: {str(e)}")
    
    def _get_default_template(self) -> List[IntentOption]:
        """ê¸°ë³¸ ì˜ë„ í…œí”Œë¦¿"""
        return [
            IntentOption(
                title="ê³„íš ì„¸ìš°ê¸°",
                description="ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ê³„íšì„ ì„¸ìš°ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                icon="ğŸ“‹"
            ),
            IntentOption(
                title="ì¤€ë¹„í•˜ê¸°",
                description="í•„ìš”í•œ ê²ƒë“¤ì„ ì¤€ë¹„í•˜ê³  ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                icon="âœ…"
            ),
            IntentOption(
                title="ì •ë³´ ì°¾ê¸°",
                description="ê´€ë ¨ëœ ì •ë³´ë¥¼ ì¡°ì‚¬í•˜ê³  ì•Œì•„ë³´ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                icon="ğŸ”"
            ),
            IntentOption(
                title="ë°”ë¡œ ì‹œì‘í•˜ê¸°",
                description="ì§€ê¸ˆ ë‹¹ì¥ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                icon="ğŸš€"
            )
        ]
    
    async def parallel_search(self, queries: List[str]) -> List[SearchResult]:
        """ì—¬ëŸ¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰"""
        logger.info("ğŸš€ GEMINI ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘")
        logger.info(f"   ğŸ“ ìš”ì²­ëœ ì¿¼ë¦¬ ìˆ˜: {len(queries)}ê°œ")
        
        if not queries:
            logger.warning("âš ï¸  ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return []
        
        # ì¿¼ë¦¬ ë‚´ìš© ë¡œê¹…
        for i, query in enumerate(queries[:5]):  # ì²˜ìŒ 5ê°œë§Œ ë¡œê¹…
            logger.info(f"   ğŸ” ì¿¼ë¦¬ {i+1}: {query}")
        if len(queries) > 5:
            logger.info(f"   ... ê·¸ ì™¸ {len(queries) - 5}ê°œ ë”")
        
        # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ìˆ˜ì— ë§ê²Œ ëª¨ë“  ì¿¼ë¦¬ ì²˜ë¦¬ (API ì œí•œ ê³ ë ¤)
        max_concurrent_searches = min(len(queries), settings.MAX_CONCURRENT_SEARCHES)
        limited_queries = queries  # ëª¨ë“  ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ë˜ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì‹¤í–‰
        
        if len(queries) > settings.MAX_CONCURRENT_SEARCHES:
            logger.info(f"ğŸ“¦ {len(queries)}ê°œ ì¿¼ë¦¬ë¥¼ {settings.MAX_CONCURRENT_SEARCHES}ê°œì”© ë°°ì¹˜ë¡œ ì²˜ë¦¬")
        else:
            logger.info(f"âœ… {len(queries)}ê°œ ì¿¼ë¦¬ ëª¨ë‘ ë³‘ë ¬ ì²˜ë¦¬")
        
        try:
            logger.info(f"âš¡ {len(limited_queries)}ê°œ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘...")
            
            # ëª¨ë“  ì¿¼ë¦¬ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì²˜ë¦¬
            all_results = []
            batch_size = settings.MAX_CONCURRENT_SEARCHES
            
            for i in range(0, len(limited_queries), batch_size):
                batch_queries = limited_queries[i:i+batch_size]
                logger.info(f"ğŸ”„ ë°°ì¹˜ {i//batch_size + 1}: {len(batch_queries)}ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘...")
                
                # ë°°ì¹˜ë³„ ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
                tasks = [self._search_single_query(query) for query in batch_queries]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                all_results.extend(batch_results)
            
            results = all_results
            
            # ì˜ˆì™¸ ì²˜ë¦¬ ë° ê²°ê³¼ ì •ë¦¬
            processed_results = []
            success_queries = []
            failed_queries = []
            
            for i, result in enumerate(results):
                query = limited_queries[i]
                if isinstance(result, Exception):
                    logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨ [{i+1}]: '{query[:50]}...' - {str(result)}")
                    processed_results.append(self._create_error_result(query, str(result)))
                    failed_queries.append(query)
                else:
                    if result.success:
                        logger.info(f"âœ… ê²€ìƒ‰ ì„±ê³µ [{i+1}]: '{query[:50]}...' ({len(result.content)}ì)")
                        success_queries.append(query)
                    else:
                        logger.warning(f"âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨ [{i+1}]: '{query[:50]}...' - {result.error_message}")
                        failed_queries.append(query)
                    processed_results.append(result)
            
            success_count = len(success_queries)
            failed_count = len(failed_queries)
            
            # ê²°ê³¼ ìš”ì•½
            logger.info("=" * 60)
            logger.info("ğŸ“Š GEMINI ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"âœ… ì„±ê³µ: {success_count}ê°œ")
            logger.info(f"âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
            logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {(success_count/len(limited_queries)*100):.1f}%")
            
            if success_count > 0:
                # ì„±ê³µí•œ ê²€ìƒ‰ ê²°ê³¼ì˜ ë‚´ìš© ê¸¸ì´ í†µê³„
                content_lengths = [len(r.content) for r in processed_results if r.success and r.content]
                if content_lengths:
                    avg_length = sum(content_lengths) / len(content_lengths)
                    min_length = min(content_lengths)
                    max_length = max(content_lengths)
                    logger.info(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: í‰ê·  {avg_length:.0f}ì (ìµœì†Œ {min_length}, ìµœëŒ€ {max_length})")
                
                # ì„±ê³µí•œ ì¿¼ë¦¬ ëª‡ ê°œ ì˜ˆì‹œ
                for query in success_queries[:3]:
                    logger.info(f"   âœ… '{query[:40]}...'")
            
            if failed_count > 0:
                logger.warning(f"âš ï¸  ì‹¤íŒ¨í•œ ì¿¼ë¦¬ {min(3, failed_count)}ê°œ ì˜ˆì‹œ:")
                for query in failed_queries[:3]:
                    logger.warning(f"   âŒ '{query[:40]}...'")
            
            logger.info("=" * 60)
            
            return processed_results
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë³‘ë ¬ ê²€ìƒ‰ ì „ì²´ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"   ğŸ”„ ëª¨ë“  ì¿¼ë¦¬ë¥¼ ì‹¤íŒ¨ ì²˜ë¦¬í•©ë‹ˆë‹¤")
            return [self._create_error_result(query, str(e)) for query in limited_queries]
    
    async def _search_single_query(self, query: str) -> SearchResult:
        """ë‹¨ì¼ ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ (Gemini ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ì‚¬ìš©)"""
        start_time = asyncio.get_event_loop().time()
        logger.debug(f"ğŸ” ë‹¨ì¼ ê²€ìƒ‰ ì‹œì‘: '{query[:50]}...'")
        
        try:
            # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì— ëŒ€í•œ êµ¬ì²´ì ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„± (Structured Output ì‚¬ìš©)
            prompt = get_search_prompt(query)
            logger.debug(f"ğŸ“ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")

            # Gemini API í˜¸ì¶œ (ì›¹ ê²€ìƒ‰ í™œì„±í™”)
            response = await self._call_gemini_api_with_search(prompt)
            elapsed = asyncio.get_event_loop().time() - start_time
            
            # ì‘ë‹µ íŒŒì‹±
            result = self._parse_search_response(query, response)
            
            if result.success:
                logger.debug(f"âœ… ê²€ìƒ‰ ì™„ë£Œ ({elapsed:.2f}ì´ˆ): {len(result.content)}ì ì‘ë‹µ")
            else:
                logger.warning(f"âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ): {result.error_message}")
            
            return result
                        
        except asyncio.TimeoutError:
            elapsed = asyncio.get_event_loop().time() - start_time
            logger.error(f"â° ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ ({elapsed:.2f}ì´ˆ)")
            logger.error(f"   ì¿¼ë¦¬: '{query[:50]}...'")
            return self._create_error_result(query, f"Search timeout after {elapsed:.2f}s")
        except Exception as e:
            elapsed = asyncio.get_event_loop().time() - start_time
            logger.error(f"ğŸ’¥ ê²€ìƒ‰ ì˜ˆì™¸ ë°œìƒ ({elapsed:.2f}ì´ˆ)")
            logger.error(f"   ì¿¼ë¦¬: '{query[:50]}...'")
            logger.error(f"   ì˜¤ë¥˜: {str(e)}")
            return self._create_error_result(query, f"Exception: {str(e)}")
    
    def _parse_search_response(self, query: str, response: str) -> SearchResult:
        """Gemini ê²€ìƒ‰ ì‘ë‹µ íŒŒì‹± (Structured Output JSON)"""
        try:
            if not response or not response.strip():
                return self._create_error_result(query, "Empty response")
            
            content = response.strip()
            logger.debug(f"Parsing structured output response: {content[:200]}...")
            
            # Structured Outputìœ¼ë¡œ ì¸í•´ ì´ë¯¸ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ì–´ì•¼ í•¨
            try:
                structured_data = json.loads(content)
                logger.info(f"Successfully parsed structured JSON response for query: {query[:50]}...")
                
                # ì‘ë‹µ êµ¬ì¡° ê²€ì¦
                if not isinstance(structured_data, dict):
                    logger.warning("Response is not a dictionary, using as-is")
                    structured_data = {"tips": [content], "contacts": [], "links": [], "price": None, "location": None}
                
                # ë§í¬ ì •ë³´ë¥¼ sourcesë¡œ ë³€í™˜
                sources = []
                if "links" in structured_data and isinstance(structured_data["links"], list):
                    for link in structured_data["links"]:
                        if isinstance(link, dict) and "url" in link:
                            sources.append(link["url"])
                        elif isinstance(link, str):
                            sources.append(link)
                
                return SearchResult(
                    query=query,
                    content=json.dumps(structured_data, ensure_ascii=False),
                    sources=sources,
                    success=True
                )
                
            except json.JSONDecodeError as json_err:
                logger.warning(f"Failed to parse structured JSON for query '{query}': {json_err}")
                logger.warning(f"Raw content: {content[:200]}...")
                
                # Structured Output ì‹¤íŒ¨ì‹œ í´ë°±
                fallback_data = {
                    "tips": [content] if content else ["ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."],
                    "contacts": [],
                    "links": [],
                    "price": None,
                    "location": None
                }
                
                return SearchResult(
                    query=query,
                    content=json.dumps(fallback_data, ensure_ascii=False),
                    sources=[],
                    success=True
                )
            
        except Exception as e:
            logger.error(f"Failed to parse Gemini structured response for query '{query}': {str(e)}")
            return self._create_error_result(query, f"Parse error: {str(e)}")
    
    def _create_error_result(self, query: str, error_message: str) -> SearchResult:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return SearchResult(
            query=query,
            content="",
            sources=[],
            success=False,
            error_message=error_message
        )
    
    def generate_search_queries_from_checklist(
        self,
        checklist_items: List[str],
        goal: str,
        answers: List[Dict[str, Any]]
    ) -> List[str]:
        """ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ì§ì ‘ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš© (1:1 ë§¤í•‘)"""
        
        logger.info("ğŸ¯ GEMINI ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì‹œì‘")
        logger.info(f"   ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ: {len(checklist_items)}ê°œ")
        logger.info(f"   ğŸ¯ ëª©í‘œ: {goal[:50]}...")
        logger.info(f"   ğŸ’¬ ë‹µë³€: {len(answers)}ê°œ")
        
        # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ì§ì ‘ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš© (í‚¤ì›Œë“œ ì¶”ì¶œ ì—†ì´)
        search_queries = []
        
        logger.info(f"   ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ì§ì ‘ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš©")
        
        for i, item in enumerate(checklist_items):
            logger.debug(f"   ğŸ” ì•„ì´í…œ {i+1}: '{item[:50]}...'")
            
            # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ê·¸ëŒ€ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš©
            # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ì—ì„œ ì´ ì•„ì´í…œì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ìš”ì²­
            search_queries.append(item)
            logger.debug(f"      â†’ ê²€ìƒ‰ ì¿¼ë¦¬: '{item}'")
        
        logger.info("=" * 60)
        logger.info("ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ â†’ ê²€ìƒ‰ ì¿¼ë¦¬ ë§¤í•‘")
        logger.info("=" * 60)
        for i, item in enumerate(checklist_items):
            logger.info(f"   {i+1:2d}. '{item[:40]}...'")
        logger.info("=" * 60)
        
        logger.info(f"âœ… 1:1 ë§¤í•‘ ì™„ë£Œ: {len(checklist_items)}ê°œ ì•„ì´í…œ â†’ {len(search_queries)}ê°œ ì¿¼ë¦¬")
        
        if not search_queries:
            logger.error("ğŸš¨ ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
            logger.error("   ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì´ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif len(search_queries) != len(checklist_items):
            logger.warning(f"âš ï¸  ì¿¼ë¦¬ ìˆ˜ ë¶ˆì¼ì¹˜: {len(checklist_items)}ê°œ ì•„ì´í…œ vs {len(search_queries)}ê°œ ì¿¼ë¦¬")
        
        return search_queries
    
    
    def _create_gemini_compatible_schema(self) -> Dict[str, Any]:
        """Gemini API í˜¸í™˜ JSON Schema ìƒì„± (SearchResponseë¥¼ ê¸°ë°˜ìœ¼ë¡œ)"""
        # GeminiëŠ” $defsì™€ $refë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¸ë¼ì¸ ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
        return {
            "type": "object",
            "properties": {
                "tips": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "ì‹¤ìš©ì ì¸ íŒê³¼ ì¡°ì–¸"
                },
                "contacts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {
                                "type": "string",
                                "description": "ì—°ë½ì²˜ ì´ë¦„"
                            },
                            "phone": {
                                "type": "string",
                                "description": "ì „í™”ë²ˆí˜¸"
                            },
                            "email": {
                                "type": "string",
                                "description": "ì´ë©”ì¼ ì£¼ì†Œ"
                            }
                        },
                        "required": ["name"]
                    },
                    "description": "ê´€ë ¨ ì—°ë½ì²˜ ì •ë³´"
                },
                "links": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "title": {
                                "type": "string",
                                "description": "ë§í¬ ì œëª©"
                            },
                            "url": {
                                "type": "string",
                                "description": "ì›¹ì‚¬ì´íŠ¸ URL"
                            }
                        },
                        "required": ["title", "url"]
                    },
                    "description": "ìœ ìš©í•œ ì›¹ì‚¬ì´íŠ¸ ë§í¬"
                },
                "price": {
                    "type": "string",
                    "description": "ì˜ˆìƒ ë¹„ìš© ë˜ëŠ” ê°€ê²© ì •ë³´"
                },
                "location": {
                    "type": "string", 
                    "description": "ìœ„ì¹˜ ë˜ëŠ” ì¥ì†Œ ì •ë³´"
                }
            },
            "required": ["tips", "contacts", "links"]
        }

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
gemini_service = GeminiService() 