"""
Gemini API ì €ìˆ˜ì¤€ í´ë¼ì´ì–¸íŠ¸

ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
- Google Gemini APIì™€ì˜ ì§ì ‘ì ì¸ í†µì‹  ë‹´ë‹¹ (Infrastructure Layer)
- ë„ë©”ì¸ ë¡œì§ê³¼ ì™¸ë¶€ API ì˜ì¡´ì„± ë¶„ë¦¬
- API í˜¸ì¶œ ë°©ì‹ ë³€ê²½ ì‹œ ì´ í´ë˜ìŠ¤ë§Œ ìˆ˜ì •í•˜ë©´ ë¨ (OCP ì›ì¹™)
- ë‹¤ë¥¸ ì„œë¹„ìŠ¤ë“¤ì€ ì´ í´ë¼ì´ì–¸íŠ¸ì—ë§Œ ì˜ì¡´ (DIP ì›ì¹™)
"""

import asyncio
import json
import logging
from typing import AsyncGenerator, Any, Dict
import google.generativeai as genai

from app.core.config import settings
from app.prompts.enhanced_prompts import get_enhanced_knowledge_prompt
from .config import GeminiConfig, GeminiAPIError, GeminiResponseError, SearchResult
from .utils import create_error_result

# ìƒˆë¡œìš´ google.genai ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹œë„
try:
    from google import genai as google_genai
    from google.genai import types
    NEW_API_AVAILABLE = True
except ImportError:
    NEW_API_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.info("New google.genai API not available, using legacy approach")

logger = logging.getLogger(__name__)


class GeminiApiClient:
    """Gemini API ì €ìˆ˜ì¤€ í´ë¼ì´ì–¸íŠ¸
    
    ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
    - Google Gemini APIì™€ì˜ ëª¨ë“  í†µì‹ ì„ ë‹´ë‹¹í•˜ëŠ” Infrastructure ê³„ì¸µ
    - ë™ê¸°/ë¹„ë™ê¸° í˜¸ì¶œ, ìŠ¤íŠ¸ë¦¬ë°, ì›¹ ê²€ìƒ‰ ë“± ë‹¤ì–‘í•œ API í˜¸ì¶œ ë°©ì‹ ì§€ì›
    - API ì‘ë‹µì˜ ì›ì‹œ ë°ì´í„°ë§Œ ë°˜í™˜í•˜ê³  ë„ë©”ì¸ ë¡œì§ì€ ìƒìœ„ ì„œë¹„ìŠ¤ì— ìœ„ì„
    - ì—°ê²° ìƒíƒœ, ì¸ì¦, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“± API ìˆ˜ì¤€ì˜ ë¬¸ì œë§Œ ì²˜ë¦¬
    """
    
    def __init__(self):
        """API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Gemini API í‚¤ ê²€ì¦ ë° ëª¨ë¸ ì´ˆê¸°í™”
        - ìƒˆë¡œìš´ google.genai í´ë¼ì´ì–¸íŠ¸ì™€ ê¸°ì¡´ generativeai í´ë¼ì´ì–¸íŠ¸ ëª¨ë‘ ì§€ì›
        - API í‚¤ ë¯¸ì„¤ì • ì‹œ ì¦‰ì‹œ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì¡°ê¸° ì‹¤íŒ¨ ê°ì§€
        - ì´ˆê¸°í™” ì„±ê³µ ì‹œ ë¡œê¹…ìœ¼ë¡œ ì„¤ì • ìƒíƒœ í™•ì¸ ê°€ëŠ¥
        """
        if not settings.GEMINI_API_KEY:
            logger.error("Cannot initialize GeminiApiClient: GEMINI_API_KEY not set")
            raise ValueError("GEMINI_API_KEY not configured")
        
        # ê¸°ì¡´ generativeai í´ë¼ì´ì–¸íŠ¸ (ì¼ë°˜ APIìš©)
        genai.configure(api_key=settings.GEMINI_API_KEY)
        self.model = genai.GenerativeModel(settings.GEMINI_MODEL)
        
        # ìƒˆë¡œìš´ google.genai í´ë¼ì´ì–¸íŠ¸ (Search groundingìš©)
        if NEW_API_AVAILABLE:
            try:
                self.new_client = google_genai.Client(api_key=settings.GEMINI_API_KEY)
                logger.info(f"GeminiApiClient initialized with model: {settings.GEMINI_MODEL} (with Search grounding support)")
            except Exception as e:
                logger.warning(f"New google.genai client initialization failed: {e}")
                self.new_client = None
                logger.info(f"GeminiApiClient initialized with model: {settings.GEMINI_MODEL} (legacy mode)")
        else:
            self.new_client = None
            logger.info(f"GeminiApiClient initialized with model: {settings.GEMINI_MODEL} (legacy mode)")
    
    async def call_api(self, prompt: str) -> str:
        """Gemini API ì¼ë°˜ í˜¸ì¶œ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ë™ê¸°ì‹ Gemini API í˜¸ì¶œë¡œ ì „ì²´ ì‘ë‹µì„ í•œ ë²ˆì— ìˆ˜ì‹ 
        - ìƒì„± ì»¨í”¼ê·¸ ì„¤ì •ìœ¼ë¡œ ìŒì„±ì˜ ë‹¤ì–‘ì„±ê³¼ í’ˆì§ˆ ì œì–´
        - ì‘ë‹µ êµ¬ì¡° ë° Safety Rating ìƒì„¸ ê²€ì¦
        - ë¹ˆ ì‘ë‹µ ë˜ëŠ” ë§¤ì„œë“œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
        """
        try:
            logger.debug(f"Sending prompt to Gemini (length: {len(prompt)} chars)")
            response = await asyncio.to_thread(
                self.model.generate_content, 
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=GeminiConfig.MAX_OUTPUT_TOKENS,
                    temperature=GeminiConfig.TEMPERATURE,
                    top_p=GeminiConfig.TOP_P,
                    top_k=GeminiConfig.TOP_K
                )
            )
            
            # ì‘ë‹µ ìƒíƒœ í™•ì¸
            if not response:
                logger.error("Gemini returned None response")
                raise GeminiAPIError("Gemini returned None response")
            
            # Safety rating ë° finish reason í™•ì¸
            self._log_response_metadata(response)
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            response_text = self._extract_text_from_response(response)
            
            logger.debug(f"Raw Gemini response (length: {len(response_text) if response_text else 0})")
            
            if not response_text or not response_text.strip():
                logger.error("Gemini returned empty or whitespace-only response")
                raise GeminiAPIError("Gemini returned empty response")
                
            return response_text
            
        except Exception as e:
            logger.error(f"Gemini API call error: {str(e)}")
            raise GeminiAPIError(f"Gemini API call failed: {str(e)}")
    
    async def call_api_with_search(self, prompt: str) -> str:
        """Gemini API ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ í¬í•¨ í˜¸ì¶œ
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Google Search grounding ê¸°ëŠ¥ì„ í™œìš©í•œ ì‹¤ì‹œê°„ ì •ë³´ ê²€ìƒ‰
        - ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ í¬í•¨ëœ Structured Output JSON ì‘ë‹µ
        - ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ enhanced knowledgeë¡œ ìë™ í´ë°±
        - grounding metadata ì •ë³´ë¡œ ê²€ìƒ‰ í’ˆì§ˆ ë° ì‹ ë¢°ì„± í™•ì¸
        """
        try:
            logger.debug(f"Calling Gemini API with search enabled (prompt length: {len(prompt)} chars)")
            
            # ìƒˆë¡œìš´ google.genai APIë¥¼ ì‚¬ìš©í•œ Google Search grounding
            if NEW_API_AVAILABLE and self.new_client:
                try:
                    logger.debug("Attempting Google Search grounding with new API")
                    
                    # Google Search grounding ë„êµ¬ ì •ì˜
                    grounding_tool = types.Tool(google_search=types.GoogleSearch())
                    
                    # ìƒì„± ì„¤ì •
                    config = types.GenerateContentConfig(
                        tools=[grounding_tool],
                        response_mime_type="application/json",
                        response_schema=self._create_search_schema_new_api()
                    )
                    
                    # ìƒˆë¡œìš´ APIë¡œ í˜¸ì¶œ
                    response = await asyncio.to_thread(
                        self.new_client.models.generate_content,
                        model=settings.GEMINI_MODEL,
                        contents=prompt,
                        config=config
                    )
                    
                    # grounding metadata í™•ì¸
                    self._log_grounding_metadata_new_api(response)
                    logger.info("âœ… Google Search grounding successful with new API")
                    
                    # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    response_text = self._extract_text_from_new_response(response)
                    
                except Exception as search_error:
                    logger.warning(f"Google Search grounding failed with new API: {search_error}")
                    logger.info("Falling back to enhanced knowledge response")
                    response_text = None
            else:
                logger.debug("New google.genai client not available, using enhanced knowledge")
                response_text = None
            
            # ìƒˆë¡œìš´ API ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
            if not response_text:
                enhanced_prompt = get_enhanced_knowledge_prompt(prompt)
                response = await asyncio.to_thread(
                    self.model.generate_content,
                    enhanced_prompt,
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=GeminiConfig.MAX_OUTPUT_TOKENS,
                        temperature=GeminiConfig.TEMPERATURE,
                        top_p=GeminiConfig.TOP_P,
                        top_k=GeminiConfig.TOP_K,
                        response_mime_type="application/json",
                        response_schema=self._create_search_schema()
                    )
                )
                response_text = self._extract_text_from_response(response)
        
            # ìµœì¢… ì‘ë‹µ ê²€ì¦
            if not response_text or not response_text.strip():
                logger.error("Gemini returned empty response")
                raise GeminiAPIError("Gemini returned empty response")
            
            logger.debug(f"Gemini search response received (length: {len(response_text)})")
            return response_text
            
        except GeminiAPIError:
            raise
        except Exception as e:
            logger.error(f"Gemini search API call error: {str(e)}")
            # ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨ì‹œ ì¼ë°˜ APIë¡œ í´ë°±
            logger.info("Falling back to regular Gemini API without search")
            return await self.call_api(prompt)
    
    async def call_api_for_checklist(self, prompt: str) -> str:
        """Gemini API ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„± ì „ìš© í˜¸ì¶œ (Structured Output)
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ì²´í¬ë¦¬ìŠ¤íŠ¸ ì „ìš© JSON ìŠ¤í‚¤ë§ˆ ì ìš©ìœ¼ë¡œ ê¹¨ë—í•œ êµ¬ì¡°í™”ëœ ì‘ë‹µ
        - ë§ˆí¬ë‹¤ìš´ ë¸”ë¡(```json) ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì‘ë‹µ
        - ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª© ê°œìˆ˜ ë° êµ¬ì¡° ë³´ì¥ (3-10ê°œ)
        - titleê³¼ description í•„ë“œ êµ¬ì¡°í™”
        """
        try:
            logger.debug(f"Calling Gemini API for checklist generation (prompt length: {len(prompt)} chars)")
            
            response = await asyncio.to_thread(
                self.model.generate_content,
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=GeminiConfig.MAX_OUTPUT_TOKENS,
                    temperature=GeminiConfig.TEMPERATURE,
                    top_p=GeminiConfig.TOP_P,
                    top_k=GeminiConfig.TOP_K,
                    response_mime_type="application/json",
                    response_schema=self._create_checklist_schema()
                )
            )
            
            # ì‘ë‹µ ì²˜ë¦¬
            if not response:
                logger.error("Gemini returned None response for checklist")
                raise GeminiAPIError("Gemini returned None response for checklist")
            
            # Safety rating ë° finish reason í™•ì¸
            self._log_response_metadata(response)
            
            response_text = self._extract_text_from_response(response)
            
            logger.debug(f"Gemini checklist response received (length: {len(response_text) if response_text else 0})")
            
            if not response_text or not response_text.strip():
                logger.error("Gemini returned empty checklist response")
                raise GeminiAPIError("Gemini returned empty checklist response")
            
            # JSON í˜•ì‹ ê²€ì¦
            try:
                import json
                parsed = json.loads(response_text)
                if 'items' not in parsed or not isinstance(parsed['items'], list):
                    raise ValueError("Invalid checklist structure")
                logger.info(f"âœ… Generated structured checklist with {len(parsed['items'])} items")
            except (json.JSONDecodeError, ValueError) as e:
                logger.error(f"Invalid JSON structure in checklist response: {e}")
                raise GeminiResponseError(f"Invalid checklist JSON structure: {e}")
                
            return response_text
            
        except (GeminiAPIError, GeminiResponseError):
            raise
        except Exception as e:
            logger.error(f"Gemini checklist API call error: {str(e)}")
            raise GeminiAPIError(f"Checklist generation failed: {str(e)}")
    
    async def call_api_stream(self, prompt: str) -> AsyncGenerator[str, None]:
        """Gemini API ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Server-Sent Events í˜•ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ë°ì´í„° ì „ì†¡
        - generator_content_stream() í•¨ìˆ˜ë¡œ ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ìˆ˜ì‹ 
        - ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì§„ë‹¨ ì •ë³´ì™€ í•¨ê»˜ ì˜ˆì™¸ ë°œìƒ
        - ê° ì²­í¬ì— ëŒ€í•œ ë¡œê¹… ë° ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨
        - Vercel ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ ìµœì í™”ëœ async ìŠ¤íŠ¸ë¦¬ë°
        """
        chunks_received = 0
        total_chars = 0
        
        try:
            logger.debug(f"Starting streaming request to Gemini (prompt length: {len(prompt)} chars)")
            
            # Gemini ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •
            generation_config = genai.types.GenerationConfig(
                max_output_tokens=GeminiConfig.MAX_OUTPUT_TOKENS,
                temperature=GeminiConfig.TEMPERATURE,
                top_p=GeminiConfig.TOP_P,
                top_k=GeminiConfig.TOP_K
            )
            
            logger.debug("âœ… Gemini streaming response initiated")
            
            # Vercel ì„œë²„ë¦¬ìŠ¤ ìµœì í™”: ì§ì ‘ì ì¸ sync ìŠ¤íŠ¸ë¦¬ë°
            try:
                response_stream = self.model.generate_content(
                    prompt,
                    generation_config=generation_config,
                    stream=True
                )
                
                # ë¹„ë™ê¸° ì²­í¬ ì²˜ë¦¬ë¡œ ë³€ê²½
                for chunk in response_stream:
                    # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ ì²´í¬ë¥¼ ìœ„í•œ yield í¬ì¸íŠ¸
                    await asyncio.sleep(0)  # Allow other coroutines to run
                    
                    chunk_text = self._extract_chunk_text(chunk)
                    
                    if chunk_text:
                        chunks_received += 1
                        total_chars += len(chunk_text)
                        
                        # ì£¼ê¸°ì ìœ¼ë¡œ ì§„í–‰ ìƒí™© ë¡œê¹…
                        if chunks_received % 5 == 0:  # ë” ìì£¼ ë¡œê¹…
                            logger.debug(f"ğŸ“Š Streaming: {chunks_received} chunks, {total_chars} chars")
                        
                        yield chunk_text
                
                logger.info(f"ğŸ“‹ Stream completed: {chunks_received} chunks, {total_chars} chars")
                
            except (BrokenPipeError, ConnectionResetError, OSError) as conn_error:
                logger.warning(f"ğŸ”Œ Client disconnected during streaming: {str(conn_error)}")
                # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€ì€ ì •ìƒì ì¸ ìƒí™©ìœ¼ë¡œ ì²˜ë¦¬
                return
                
        except Exception as e:
            logger.error(f"ğŸš¨ Streaming API error: {str(e)}")
            logger.debug(f"Error details - Chunks received: {chunks_received}, Total chars: {total_chars}")
            raise GeminiAPIError(f"Gemini streaming failed: {str(e)}")
    
    def _extract_text_from_response(self, response) -> str:
        """ì‘ë‹µ ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Gemini API ì‘ë‹µì˜ ë‹¤ì–‘í•œ êµ¬ì¡°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        - response.text ì†ì„±ì´ ì—†ëŠ” ê²½ìš° candidatesì—ì„œ ì¶”ì¶œ
        - ì—¬ëŸ¬ candidateê°€ ìˆëŠ” ê²½ìš° ì²« ë²ˆì§¸ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ì‚¬ìš©
        - ëª¨ë“  ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
        """
        if hasattr(response, 'text') and response.text:
            return response.text
        
        # ëŒ€ì•ˆìœ¼ë¡œ candidatesì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        if hasattr(response, 'candidates') and response.candidates:
            for candidate in response.candidates:
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        if hasattr(part, 'text') and part.text:
                            logger.debug(f"Found text in candidate.content.parts")
                            return part.text
        
        raise GeminiAPIError("Gemini response has no extractable text")
    
    def _extract_chunk_text(self, chunk) -> str:
        """ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì˜ ê°œë³„ ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        - chunk.textê°€ ìˆìœ¼ë©´ ì§ì ‘ ì‚¬ìš©
        - ì—†ìœ¼ë©´ candidates êµ¬ì¡°ì—ì„œ ì¶”ì¶œ
        - ë¹ˆ ì²­í¬ì˜ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (ì •ìƒ)
        """
        chunk_text = ""
        
        if hasattr(chunk, 'text') and chunk.text:
            chunk_text = chunk.text
        elif hasattr(chunk, 'candidates') and chunk.candidates:
            for candidate in chunk.candidates:
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        if hasattr(part, 'text') and part.text:
                            chunk_text += part.text
        
        return chunk_text
    
    def _log_response_metadata(self, response):
        """ì‘ë‹µ ë©”íƒ€ë°ì´í„° ë¡œê¹… (Safety Rating, Finish Reason ë“±)
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - API ì‘ë‹µì˜ í’ˆì§ˆ ë° ì•ˆì „ì„± ì •ë³´ ê¸°ë¡
        - finish_reasonìœ¼ë¡œ ì‘ë‹µ ì™„ë£Œ/ì¤‘ë‹¨ ì›ì¸ íŒŒì•…
        - safety_ratingsë¡œ ì»¨í…ì¸  í•„í„°ë§ ì •ë³´ í™•ì¸
        - ë””ë²„ê¹… ë° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ì— í™œìš©
        """
        if hasattr(response, 'candidates') and response.candidates:
            for i, candidate in enumerate(response.candidates):
                finish_reason = getattr(candidate, 'finish_reason', 'N/A')
                logger.debug(f"Candidate {i} finish_reason: {finish_reason}")
                
                # finish_reason í•´ì„
                if finish_reason == 2:
                    logger.warning("Response was truncated due to MAX_TOKENS limit")
                elif finish_reason == 3:
                    logger.warning("Response was blocked by safety filters")
                elif finish_reason == 4:
                    logger.warning("Response was blocked due to recitation concerns")
                
                if hasattr(candidate, 'safety_ratings'):
                    logger.debug(f"Candidate {i} safety_ratings: {candidate.safety_ratings}")
    
    def _log_grounding_metadata(self, response):
        """Grounding ë©”íƒ€ë°ì´í„° ë¡œê¹… (ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì •ë³´)
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ì‚¬ìš© ì‹œ ê²€ìƒ‰ í’ˆì§ˆ ì •ë³´ ê¸°ë¡
        - grounding_chunksë¡œ ê²€ìƒ‰ëœ ì†ŒìŠ¤ ê°œìˆ˜ í™•ì¸
        - search_entry_pointë¡œ ê²€ìƒ‰ ì§„ì…ì  ì •ë³´ ê¸°ë¡
        - ê²€ìƒ‰ ê²°ê³¼ì˜ ì‹ ë¢°ì„± í‰ê°€ì— í™œìš©
        """
        if hasattr(response, 'candidates') and response.candidates:
            for candidate in response.candidates:
                if hasattr(candidate, 'grounding_metadata'):
                    logger.info("Response includes grounding metadata (web search results)")
                    if hasattr(candidate.grounding_metadata, 'search_entry_point'):
                        logger.debug(f"Search entry point: {candidate.grounding_metadata.search_entry_point}")
                    if hasattr(candidate.grounding_metadata, 'grounding_chunks'):
                        logger.debug(f"Found {len(candidate.grounding_metadata.grounding_chunks)} grounding chunks")
    
    def _create_checklist_schema(self) -> Dict[str, Any]:
        """ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„±ìš© JSON ìŠ¤í‚¤ë§ˆ (Gemini Structured Output í˜¸í™˜)
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©ë“¤ì„ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ì‘ë‹µë°›ê¸° ìœ„í•œ ìŠ¤í‚¤ë§ˆ
        - ê° í•­ëª©ì€ title(í•„ìˆ˜)ê³¼ description(ì„ íƒ) í¬í•¨
        - ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì—†ì´ ê¹¨ë—í•œ JSONë§Œ ì‘ë‹µ
        - Gemini API Structured Output ì™„ì „ í˜¸í™˜ í˜•íƒœ
        """
        return {
            "type": "object",
            "properties": {
                "items": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "title": {"type": "string"},
                            "description": {"type": "string"}
                        },
                        "required": ["title"]
                    }
                }
            },
            "required": ["items"]
        }

    def _create_search_schema(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ ì‘ë‹µìš© JSON ìŠ¤í‚¤ë§ˆ ìƒì„± (Gemini Structured Output í˜¸í™˜)
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Structured Outputì„ ìœ„í•œ Gemini API í˜¸í™˜ JSON ìŠ¤í‚¤ë§ˆ
        - ê°„ë‹¨í•˜ê³  ëª…í™•í•œ êµ¬ì¡°ë¡œ ì•ˆì •ì„± ë³´ì¥
        - steps, contacts, links, price, location ë“± ê²€ìƒ‰ ê²°ê³¼ í•„ë“œ ì •ì˜
        """
        return {
            "type": "object",
            "properties": {
                "steps": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "contacts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string"},
                            "phone": {"type": "string"},
                            "email": {"type": "string"}
                        },
                        "required": ["name"]
                    }
                },
                "links": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "title": {"type": "string"},
                            "url": {"type": "string"}
                        },
                        "required": ["title", "url"]
                    }
                },
                "price": {"type": "string"}
            },
            "required": ["steps", "contacts", "links"]
        }
    
    def _create_search_schema_new_api(self) -> Dict[str, Any]:
        """ìƒˆë¡œìš´ google.genai APIìš© ê²€ìƒ‰ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
        if not NEW_API_AVAILABLE:
            return self._create_search_schema()
            
        return {
            "type": "object",
            "properties": {
                "steps": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "contacts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string"},
                            "phone": {"type": "string"},
                            "email": {"type": "string"}
                        },
                        "required": ["name"]
                    }
                },
                "links": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "title": {"type": "string"},
                            "url": {"type": "string"}
                        },
                        "required": ["title", "url"]
                    }
                },
                "price": {"type": "string"}
            },
            "required": ["steps", "contacts", "links"]
        }
    
    def _extract_text_from_new_response(self, response) -> str:
        """ìƒˆë¡œìš´ google.genai API ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not NEW_API_AVAILABLE:
            return ""
            
        try:
            if hasattr(response, 'text') and response.text:
                return response.text
            
            # candidates êµ¬ì¡°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            if hasattr(response, 'candidates') and response.candidates:
                for candidate in response.candidates:
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                return part.text
            
            logger.warning("Could not extract text from new API response")
            return ""
            
        except Exception as e:
            logger.error(f"Error extracting text from new API response: {e}")
            return ""
    
    def _log_grounding_metadata_new_api(self, response):
        """ìƒˆë¡œìš´ APIì˜ grounding metadata ë¡œê¹…"""
        if not NEW_API_AVAILABLE:
            return
            
        try:
            if hasattr(response, 'candidates') and response.candidates:
                for candidate in response.candidates:
                    if hasattr(candidate, 'grounding_metadata'):
                        metadata = candidate.grounding_metadata
                        logger.info("âœ… Response includes Google Search grounding metadata")
                        
                        if hasattr(metadata, 'web_search_queries'):
                            logger.debug(f"Search queries: {metadata.web_search_queries}")
                        
                        if hasattr(metadata, 'grounding_chunks'):
                            logger.debug(f"Found {len(metadata.grounding_chunks)} grounding chunks")
                            
                        if hasattr(metadata, 'grounding_supports'):
                            logger.debug(f"Found {len(metadata.grounding_supports)} grounding supports")
                            
        except Exception as e:
            logger.debug(f"Could not parse grounding metadata: {e}")