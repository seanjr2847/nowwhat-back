"""
ê²€ìƒ‰ ê¸°ëŠ¥ ì „ìš© ì„œë¹„ìŠ¤

ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
- ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ê¸°ë°˜ ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ì „ë‹´
- ë‹¤ì¤‘ ê²€ìƒ‰ ì¿¼ë¦¬ì˜ ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
- ê²€ìƒ‰ ê²°ê³¼ì˜ êµ¬ì¡°í™” ë° í’ˆì§ˆ ê´€ë¦¬
- ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œì—ë„ ì•ˆì •ì ì¸ ê²°ê³¼ ë°˜í™˜
"""

import asyncio
import json
import logging
from typing import List, Dict, Any

from app.core.config import settings
from app.prompts.search_prompts import get_search_prompt
from .api_client import GeminiApiClient
from .config import GeminiConfig, SearchResult
from .utils import create_error_result

logger = logging.getLogger(__name__)


class SearchService:
    """ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ì „ìš© ì„œë¹„ìŠ¤ (SRP)
    
    ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
    - ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œë³„ë¡œ ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë³‘ë ¬ ì²˜ë¦¬
    - API ì œí•œ(MAX_CONCURRENT_SEARCHES)ì„ ê³ ë ¤í•œ ë°°ì¹˜ ì²˜ë¦¬
    - ê° ë°°ì¹˜ë³„ë¡œ asyncio.gatherë¡œ ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
    - ì„±ê³µ/ì‹¤íŒ¨ í†µê³„ ë° ë¡œê¹…ìœ¼ë¡œ ê²€ìƒ‰ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
    - ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œì—ë„ ì˜¤ë¥˜ ê²°ê³¼ ê°ì²´ë¡œ ì „ì²´ ê²°ê³¼ì— í¬í•¨
    """
    
    def __init__(self, api_client: GeminiApiClient):
        """ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            api_client: Gemini API í´ë¼ì´ì–¸íŠ¸ (DIP - ì˜ì¡´ì„± ì£¼ì…)
        """
        self.api_client = api_client
        logger.info("SearchService initialized")
    
    async def parallel_search(self, queries: List[str]) -> List[SearchResult]:
        """ë‹¤ì¤‘ ê²€ìƒ‰ ì¿¼ë¦¬ ë³‘ë ¬ ì‹¤í–‰
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œë³„ë¡œ ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë³‘ë ¬ ì²˜ë¦¬
        - API ì œí•œ(MAX_CONCURRENT_SEARCHES)ì„ ê³ ë ¤í•œ ë°°ì¹˜ ì²˜ë¦¬
        - ê° ë°°ì¹˜ë³„ë¡œ asyncio.gatherë¡œ ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
        - ì„±ê³µ/ì‹¤íŒ¨ í†µê³„ ë° ë¡œê¹…ìœ¼ë¡œ ê²€ìƒ‰ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
        - ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œì—ë„ ì˜¤ë¥˜ ê²°ê³¼ ê°ì²´ë¡œ ì „ì²´ ê²°ê³¼ì— í¬í•¨
        
        Args:
            queries: ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[SearchResult]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì„±ê³µ/ì‹¤íŒ¨ ëª¨ë‘ í¬í•¨)
        """
        logger.info("ğŸš€ GEMINI ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘")
        logger.info(f"   ğŸ“ ìš”ì²­ëœ ì¿¼ë¦¬ ìˆ˜: {len(queries)}ê°œ")
        
        if not queries:
            logger.warning("âš ï¸  ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return []
        
        # ì¿¼ë¦¬ ë‚´ìš© ë¡œê¹… (ì²˜ìŒ 5ê°œë§Œ)
        for i, query in enumerate(queries[:5]):
            logger.info(f"   ğŸ” ì¿¼ë¦¬ {i+1}: {query}")
        if len(queries) > 5:
            logger.info(f"   ... ê·¸ ì™¸ {len(queries) - 5}ê°œ ë”")
        
        try:
            # ë°°ì¹˜ë³„ ë³‘ë ¬ ì²˜ë¦¬ë¡œ API ì œí•œ ì¤€ìˆ˜
            all_results = await self._execute_batched_searches(queries)
            
            # ê²°ê³¼ ë¶„ì„ ë° ë¡œê¹…
            processed_results = self._process_search_results(queries, all_results)
            self._log_search_summary(queries, processed_results)
            
            return processed_results
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë³‘ë ¬ ê²€ìƒ‰ ì „ì²´ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"   ğŸ”„ ëª¨ë“  ì¿¼ë¦¬ë¥¼ ì‹¤íŒ¨ ì²˜ë¦¬í•©ë‹ˆë‹¤")
            return [create_error_result(query, str(e)) for query in queries]
    
    async def _execute_batched_searches(self, queries: List[str]) -> List[Any]:
        """ë°°ì¹˜ë³„ ê²€ìƒ‰ ì‹¤í–‰
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ì „ì²´ ì¿¼ë¦¬ë¥¼ API ì œí•œì— ë§ì¶° ë°°ì¹˜ë¡œ ë¶„í• 
        - ê° ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ìµœëŒ€ ì„±ëŠ¥ í™•ë³´
        - ë°°ì¹˜ ê°„ ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ API ì œí•œ ì¤€ìˆ˜
        - ëª¨ë“  ë°°ì¹˜ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ë°˜í™˜
        """
        all_results = []
        batch_size = settings.MAX_CONCURRENT_SEARCHES
        
        logger.info(f"ğŸ“¦ {len(queries)}ê°œ ì¿¼ë¦¬ë¥¼ {batch_size}ê°œì”© ë°°ì¹˜ë¡œ ì²˜ë¦¬")
        
        for i in range(0, len(queries), batch_size):
            batch_queries = queries[i:i+batch_size]
            batch_number = i // batch_size + 1
            
            logger.info(f"ğŸ”„ ë°°ì¹˜ {batch_number}: {len(batch_queries)}ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ë³„ ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
            tasks = [self._search_single_query(query) for query in batch_queries]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            all_results.extend(batch_results)
        
        return all_results
    
    def _process_search_results(self, queries: List[str], raw_results: List[Any]) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë° ë¶„ë¥˜
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ì›ì‹œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ SearchResult ê°ì²´ë¡œ ë³€í™˜
        - ì˜ˆì™¸ ë°œìƒí•œ ê²€ìƒ‰ì€ ì˜¤ë¥˜ ê²°ê³¼ ê°ì²´ë¡œ ë³€í™˜
        - ì„±ê³µ/ì‹¤íŒ¨ ì¿¼ë¦¬ë¥¼ ë¶„ë¥˜í•˜ì—¬ í†µê³„ ìƒì„±
        - ëª¨ë“  ê²€ìƒ‰ì— ëŒ€í•´ ì¼ê´€ëœ ê²°ê³¼ êµ¬ì¡° ë³´ì¥
        """
        processed_results = []
        success_queries = []
        failed_queries = []
        
        for i, result in enumerate(raw_results):
            query = queries[i]
            
            if isinstance(result, Exception):
                logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨ [{i+1}]: '{query[:50]}...' - {str(result)}")
                processed_results.append(create_error_result(query, str(result)))
                failed_queries.append(query)
            else:
                if result.success:
                    logger.info(f"âœ… ê²€ìƒ‰ ì„±ê³µ [{i+1}]: '{query[:50]}...' ({len(result.content)}ì)")
                    success_queries.append(query)
                else:
                    logger.warning(f"âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨ [{i+1}]: '{query[:50]}...' - {result.error_message}")
                    failed_queries.append(query)
                processed_results.append(result)
        
        return processed_results
    
    def _log_search_summary(self, queries: List[str], results: List[SearchResult]):
        """ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë¡œê¹…
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ì „ì²´ ê²€ìƒ‰ ì„¸ì…˜ì˜ ì„±ê³µë¥  ë° í†µê³„ ì •ë³´ ì œê³µ
        - ì„±ê³µí•œ ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆ ì§€í‘œ (ì‘ë‹µ ê¸¸ì´ ë“±) ë¶„ì„
        - ì‹¤íŒ¨í•œ ê²€ìƒ‰ì˜ ì˜ˆì‹œë¥¼ í†µí•œ ë¬¸ì œì  íŒŒì•…
        - ìš´ì˜ ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë©”íŠ¸ë¦­ ì œê³µ
        """
        success_count = len([r for r in results if r.success])
        failed_count = len(results) - success_count
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("=" * 60)
        logger.info("ğŸ“Š GEMINI ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 60)
        logger.info(f"âœ… ì„±ê³µ: {success_count}ê°œ")
        logger.info(f"âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
        logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {(success_count/len(queries)*100):.1f}%")
        
        if success_count > 0:
            # ì„±ê³µí•œ ê²€ìƒ‰ ê²°ê³¼ì˜ ë‚´ìš© ê¸¸ì´ í†µê³„
            content_lengths = [len(r.content) for r in results if r.success and r.content]
            if content_lengths:
                avg_length = sum(content_lengths) / len(content_lengths)
                min_length = min(content_lengths)
                max_length = max(content_lengths)
                logger.info(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: í‰ê·  {avg_length:.0f}ì (ìµœì†Œ {min_length}, ìµœëŒ€ {max_length})")
            
            # ì„±ê³µí•œ ì¿¼ë¦¬ ëª‡ ê°œ ì˜ˆì‹œ
            success_examples = [q for q, r in zip(queries, results) if r.success][:3]
            for query in success_examples:
                logger.info(f"   âœ… '{query[:40]}...'")
        
        if failed_count > 0:
            failed_examples = [q for q, r in zip(queries, results) if not r.success][:3]
            logger.warning(f"âš ï¸  ì‹¤íŒ¨í•œ ì¿¼ë¦¬ {min(3, failed_count)}ê°œ ì˜ˆì‹œ:")
            for query in failed_examples:
                logger.warning(f"   âŒ '{query[:40]}...'")
        
        logger.info("=" * 60)
    
    async def _search_single_query(self, query: str) -> SearchResult:
        """ë‹¨ì¼ ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤ì‹œê°„ ì‹¤í–‰
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ê°œë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì— ëŒ€í•œ Google ê²€ìƒ‰ ê¸°ë°˜ ì‹¤ì‹œê°„ ì •ë³´ ìˆ˜ì§‘
        - get_search_prompt()ë¡œ êµ¬ì¡°í™”ëœ ê²€ìƒ‰ í”„ë¡¬í”„íŠ¸ ìƒì„±
        - Gemini APIì˜ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ìœ¼ë¡œ ìµœì‹  ì •ë³´ íšë“
        - ê²€ìƒ‰ ì‹œê°„ ì¶”ì  ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        - ì˜ˆì™¸ ë°œìƒ ì‹œ SearchResult ì˜¤ë¥˜ ê°ì²´ë¡œ ì•ˆì „í•œ ì‹¤íŒ¨ ì²˜ë¦¬
        """
        start_time = asyncio.get_event_loop().time()
        logger.debug(f"ğŸ” ë‹¨ì¼ ê²€ìƒ‰ ì‹œì‘: '{query[:50]}...'")
        
        try:
            # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì— ëŒ€í•œ êµ¬ì²´ì ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = get_search_prompt(query)
            logger.debug(f"ğŸ“ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")

            # Gemini API í˜¸ì¶œ (ì›¹ ê²€ìƒ‰ í™œì„±í™”)
            response = await self.api_client.call_api_with_search(prompt)
            elapsed = asyncio.get_event_loop().time() - start_time
            
            # ì‘ë‹µ íŒŒì‹±
            result = self._parse_search_response(query, response)
            
            if result.success:
                logger.debug(f"âœ… ê²€ìƒ‰ ì™„ë£Œ ({elapsed:.2f}ì´ˆ): {len(result.content)}ì ì‘ë‹µ")
            else:
                logger.warning(f"âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ): {result.error_message}")
            
            return result
                        
        except asyncio.TimeoutError:
            elapsed = asyncio.get_event_loop().time() - start_time
            logger.error(f"â° ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ ({elapsed:.2f}ì´ˆ)")
            logger.error(f"   ì¿¼ë¦¬: '{query[:50]}...'")
            return create_error_result(query, f"Search timeout after {elapsed:.2f}s")
        except Exception as e:
            elapsed = asyncio.get_event_loop().time() - start_time
            logger.error(f"ğŸ’¥ ê²€ìƒ‰ ì˜ˆì™¸ ë°œìƒ ({elapsed:.2f}ì´ˆ)")
            logger.error(f"   ì¿¼ë¦¬: '{query[:50]}...'")
            logger.error(f"   ì˜¤ë¥˜: {str(e)}")
            return create_error_result(query, f"Exception: {str(e)}")
    
    def _parse_search_response(self, query: str, response: str) -> SearchResult:
        """Gemini ì›¹ ê²€ìƒ‰ ì‘ë‹µ êµ¬ì¡°í™” íŒŒì‹±
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - Structured Outputìœ¼ë¡œ ìˆ˜ì‹ ëœ JSON í˜•íƒœì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ SearchResult ê°ì²´ë¡œ ë³€í™˜
        - tips, contacts, links, price, location ë“± ë‹¤ì–‘í•œ ì •ë³´ ìœ í˜• ì²˜ë¦¬
        - JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì»¨í…ì¸ ë¥¼ í´ë°± ë°ì´í„°ë¡œ í™œìš©
        - ë§í¬ ì •ë³´ë¥¼ sources ë°°ì—´ë¡œ ì¶”ì¶œí•˜ì—¬ ì†ë§‰ ì¶”ì 
        - ëª¨ë“  ê²½ìš°ì— ìœ íš¨í•œ SearchResult ê°ì²´ ë°˜í™˜ ë³´ì¥
        """
        try:
            if not response or not response.strip():
                return create_error_result(query, "Empty response")
            
            content = response.strip()
            logger.debug(f"Parsing structured output response: {content[:200]}...")
            
            # Structured Outputìœ¼ë¡œ ì¸í•´ ì´ë¯¸ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ì–´ì•¼ í•¨
            try:
                structured_data = json.loads(content)
                logger.info(f"Successfully parsed structured JSON response for query: {query[:50]}...")
                
                # ì‘ë‹µ êµ¬ì¡° ê²€ì¦
                if not isinstance(structured_data, dict):
                    logger.warning("Response is not a dictionary, using as-is")
                    structured_data = {"tips": [content], "contacts": [], "links": [], "price": None, "location": None}
                
                # ë§í¬ ì •ë³´ë¥¼ sourcesë¡œ ë³€í™˜
                sources = []
                if "links" in structured_data and isinstance(structured_data["links"], list):
                    for link in structured_data["links"]:
                        if isinstance(link, dict) and "url" in link:
                            sources.append(link["url"])
                        elif isinstance(link, str):
                            sources.append(link)
                
                return SearchResult(
                    query=query,
                    content=json.dumps(structured_data, ensure_ascii=False),
                    sources=sources,
                    success=True
                )
                
            except json.JSONDecodeError as json_err:
                logger.warning(f"Failed to parse structured JSON for query '{query}': {json_err}")
                logger.warning(f"Raw content: {content[:200]}...")
                
                # Structured Output ì‹¤íŒ¨ì‹œ í´ë°±
                fallback_data = {
                    "tips": [content] if content else ["ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."],
                    "contacts": [],
                    "links": [],
                    "price": None,
                    "location": None
                }
                
                return SearchResult(
                    query=query,
                    content=json.dumps(fallback_data, ensure_ascii=False),
                    sources=[],
                    success=True
                )
            
        except Exception as e:
            logger.error(f"Failed to parse Gemini structured response for query '{query}': {str(e)}")
            return create_error_result(query, f"Parse error: {str(e)}")
    
    def generate_search_queries_from_checklist(
        self,
        checklist_items: List[str],
        goal: str,
        answers: List[Dict[str, Any]]
    ) -> List[str]:
        """ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ê¸°ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§:
        - ìƒì„±ëœ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ê°ê°ì„ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ 1:1 ë³€í™˜
        - ì‚¬ìš©ìì˜ ëª©í‘œì™€ ë‹µë³€ ë§¥ë½ì„ ê³ ë ¤í•œ ì¿¼ë¦¬ ìµœì í™”
        - ê° ì•„ì´í…œì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ì œê³µ
        - ëª¨ë“  ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì— ëŒ€í•œ ì¿¼ë¦¬ ë³´ì¥ìœ¼ë¡œ ì™„ì „í•œ ê²€ìƒ‰ ë²”ìœ„
        
        Args:
            checklist_items: ìƒì„±ëœ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
            goal: ì‚¬ìš©ì ëª©í‘œ
            answers: ì‚¬ìš©ì ë‹µë³€ ë°ì´í„°
            
        Returns:
            List[str]: ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("ğŸ¯ GEMINI ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì‹œì‘")
        logger.info(f"   ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ: {len(checklist_items)}ê°œ")
        logger.info(f"   ğŸ¯ ëª©í‘œ: {goal[:50]}...")
        logger.info(f"   ğŸ’¬ ë‹µë³€: {len(answers)}ê°œ")
        
        # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ê·¸ëŒ€ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš© (1:1 ë§¤í•‘)
        search_queries = []
        
        for i, item in enumerate(checklist_items):
            if item and item.strip():
                # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜
                # ì˜ˆ: "ì—¬í–‰ ë³´í—˜ ê°€ì…í•˜ê¸°" -> "ì—¬í–‰ ë³´í—˜ ì¢…ë¥˜ ë¹„êµ ê°€ì… ë°©ë²•"
                search_query = item.strip()
                
                # ê¸°ë³¸ì ì¸ ê²€ìƒ‰ í‚¤ì›Œë“œ ìµœì í™” (ì„ íƒì )
                if "í•˜ê¸°" in search_query:
                    search_query = search_query.replace("í•˜ê¸°", "ë°©ë²•")
                if "ì¤€ë¹„" in search_query:
                    search_query += " ì²´í¬ë¦¬ìŠ¤íŠ¸"
                
                search_queries.append(search_query)
                logger.debug(f"   ğŸ” ì¿¼ë¦¬ {i+1}: {search_query}")
            else:
                logger.warning(f"   âš ï¸  ë¹ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ê±´ë„ˆë›°ê¸°: ì¸ë±ìŠ¤ {i}")
        
        logger.info(f"âœ… 1:1 ë§¤í•‘ ì™„ë£Œ: {len(checklist_items)}ê°œ ì•„ì´í…œ â†’ {len(search_queries)}ê°œ ì¿¼ë¦¬")
        
        if not search_queries:
            logger.error("ğŸš¨ ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
            logger.error("   ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì´ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif len(search_queries) != len(checklist_items):
            logger.warning(f"âš ï¸  ì¿¼ë¦¬ ìˆ˜ ë¶ˆì¼ì¹˜: {len(checklist_items)}ê°œ ì•„ì´í…œ vs {len(search_queries)}ê°œ ì¿¼ë¦¬")
        
        return search_queries