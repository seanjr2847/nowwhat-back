import asyncio
import aiohttp
import logging
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from app.core.config import settings

logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """Perplexity API ê²€ìƒ‰ ê²°ê³¼"""
    query: str
    content: str
    sources: List[str]
    success: bool
    error_message: Optional[str] = None

class PerplexityService:
    """Perplexity APIë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì •ë³´ ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.api_url = "https://api.perplexity.ai/chat/completions"
        self.api_key = getattr(settings, 'PERPLEXITY_API_KEY', '')
        self.max_concurrent_searches = getattr(settings, 'MAX_CONCURRENT_SEARCHES', 10)
        self.timeout_seconds = getattr(settings, 'SEARCH_TIMEOUT_SECONDS', 15)
        
        # API í‚¤ ìƒíƒœ ë¡œê¹…
        if not self.api_key:
            logger.error("ğŸš¨ PERPLEXITY_API_KEY not found in environment variables!")
            logger.error("   ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì— descriptionì´ ì¶”ê°€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            logger.error("   í™˜ê²½ë³€ìˆ˜ PERPLEXITY_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        else:
            logger.info(f"âœ… Perplexity API í‚¤ í™•ì¸ë¨ (ê¸¸ì´: {len(self.api_key)} ë¬¸ì)")
            logger.info(f"   ìµœëŒ€ ë™ì‹œ ê²€ìƒ‰: {self.max_concurrent_searches}ê°œ")
            logger.info(f"   ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ: {self.timeout_seconds}ì´ˆ")
    
    async def parallel_search(self, queries: List[str]) -> List[SearchResult]:
        """10ê°œì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰"""
        if not self.api_key:
            logger.error("ğŸš¨ Perplexity API í‚¤ê°€ ì—†ì–´ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤")
            logger.error(f"   {len(queries)}ê°œ ì¿¼ë¦¬: {', '.join(queries[:3])}{'...' if len(queries) > 3 else ''}")
            return [self._create_empty_result(query) for query in queries]
        
        if not queries:
            return []
        
        # ìµœëŒ€ ë™ì‹œ ê²€ìƒ‰ ìˆ˜ ì œí•œ
        limited_queries = queries[:self.max_concurrent_searches]
        
        try:
            # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
            tasks = [self._search_single_query(query) for query in limited_queries]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ì˜ˆì™¸ ì²˜ë¦¬ ë° ê²°ê³¼ ì •ë¦¬
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Search failed for query '{limited_queries[i]}': {str(result)}")
                    processed_results.append(self._create_error_result(limited_queries[i], str(result)))
                else:
                    processed_results.append(result)
            
            success_count = sum(1 for r in processed_results if r.success)
            failed_count = len(limited_queries) - success_count
            
            if success_count > 0:
                logger.info(f"ğŸ” ê²€ìƒ‰ ì™„ë£Œ: {success_count}/{len(limited_queries)}ê°œ ì„±ê³µ")
                # ì„±ê³µí•œ ê²€ìƒ‰ ê²°ê³¼ì˜ ë‚´ìš© ê¸¸ì´ ë¡œê¹…
                content_lengths = [len(r.content) for r in processed_results if r.success and r.content]
                if content_lengths:
                    avg_length = sum(content_lengths) / len(content_lengths)
                    logger.info(f"   í‰ê·  ì‘ë‹µ ê¸¸ì´: {avg_length:.0f}ì")
            else:
                logger.warning(f"âš ï¸ ëª¨ë“  ê²€ìƒ‰ ì‹¤íŒ¨: {failed_count}ê°œ ì‹¤íŒ¨")
                logger.warning("   ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì— descriptionì´ ì¶”ê°€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            
            return processed_results
            
        except Exception as e:
            logger.error(f"Parallel search failed: {str(e)}")
            return [self._create_error_result(query, str(e)) for query in limited_queries]
    
    async def _search_single_query(self, query: str) -> SearchResult:
        """ë‹¨ì¼ ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "llama-3.1-sonar-small-128k-online",
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a helpful assistant that provides current, accurate information. Provide concise, factual answers with reliable sources."
                    },
                    {
                        "role": "user", 
                        "content": query
                    }
                ],
                "max_tokens": 300,
                "temperature": 0.2,
                "stream": False
            }
            
            timeout = aiohttp.ClientTimeout(total=self.timeout_seconds)
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(self.api_url, json=payload, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        return self._parse_perplexity_response(query, data)
                    else:
                        error_text = await response.text()
                        logger.error(f"ğŸš¨ Perplexity API ì˜¤ë¥˜ {response.status} (ì¿¼ë¦¬: '{query[:30]}...')")
                        logger.error(f"   ì‘ë‹µ: {error_text[:100]}...")
                        return self._create_error_result(query, f"API error {response.status}")
                        
        except asyncio.TimeoutError:
            logger.error(f"Search timeout for query: {query}")
            return self._create_error_result(query, "Search timeout")
        except Exception as e:
            logger.error(f"Search failed for query '{query}': {str(e)}")
            return self._create_error_result(query, str(e))
    
    def _parse_perplexity_response(self, query: str, data: Dict[str, Any]) -> SearchResult:
        """Perplexity API ì‘ë‹µ íŒŒì‹±"""
        try:
            if "choices" not in data or not data["choices"]:
                return self._create_error_result(query, "No choices in response")
            
            choice = data["choices"][0]
            if "message" not in choice:
                return self._create_error_result(query, "No message in choice")
            
            content = choice["message"].get("content", "").strip()
            if not content:
                return self._create_error_result(query, "Empty content in response")
            
            # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ (PerplexityëŠ” ë³´í†µ ì‘ë‹µì— ì†ŒìŠ¤ë¥¼ í¬í•¨)
            sources = self._extract_sources_from_content(content)
            
            return SearchResult(
                query=query,
                content=content,
                sources=sources,
                success=True
            )
            
        except Exception as e:
            logger.error(f"Failed to parse Perplexity response for query '{query}': {str(e)}")
            return self._create_error_result(query, f"Parse error: {str(e)}")
    
    def _extract_sources_from_content(self, content: str) -> List[str]:
        """ì½˜í…ì¸ ì—ì„œ ì†ŒìŠ¤ URL ì¶”ì¶œ"""
        import re
        
        # URL íŒ¨í„´ ë§¤ì¹­
        url_pattern = r'https?://[^\s\])]+'
        sources = re.findall(url_pattern, content)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        unique_sources = list(set(sources))
        return unique_sources[:5]  # ìµœëŒ€ 5ê°œ ì†ŒìŠ¤
    
    def _create_empty_result(self, query: str) -> SearchResult:
        """ë¹ˆ ê²°ê³¼ ìƒì„± (API í‚¤ ì—†ìŒ)"""
        return SearchResult(
            query=query,
            content="ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            sources=[],
            success=False,
            error_message="Perplexity API key not configured"
        )
    
    def _create_error_result(self, query: str, error_message: str) -> SearchResult:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return SearchResult(
            query=query,
            content="",
            sources=[],
            success=False,
            error_message=error_message
        )
    
    def generate_search_queries_from_checklist(
        self,
        checklist_items: List[str],
        goal: str,
        answers: List[Dict[str, Any]]
    ) -> List[str]:
        """ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ë²”ìš©ì  ë°©ì‹)"""
        
        # ë‹µë³€ì—ì„œ í•µì‹¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        answer_context = self._extract_answer_context(answers)
        
        search_queries = []
        
        # ê° ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        for item in checklist_items[:10]:  # ìµœëŒ€ 10ê°œ ì•„ì´í…œ
            # ì•„ì´í…œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            core_keywords = self._extract_core_keywords_from_item(item)
            
            if core_keywords:
                # ì—¬ëŸ¬ íŒ¨í„´ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                queries = self._generate_item_specific_queries(core_keywords, answer_context)
                search_queries.extend(queries)
        
        # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì œí•œ
        unique_queries = list(dict.fromkeys(search_queries))[:15]  # ìµœëŒ€ 15ê°œ
        
        logger.info(f"Generated {len(unique_queries)} search queries from {len(checklist_items)} checklist items")
        return unique_queries
    
    def _extract_core_keywords_from_item(self, item: str) -> List[str]:
        """ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì—ì„œ ê²€ìƒ‰ì— ìœ ìš©í•œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        import re
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = [
            'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì˜', 'ì—', 'ì—ì„œ', 'ì™€', 'ê³¼',
            'í•˜ê¸°', 'í•˜ì„¸ìš”', 'í•©ë‹ˆë‹¤', 'ìœ„í•œ', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•œ', 'í•¨ê»˜'
        ]
        
        # ëª…ì‚¬í˜• í‚¤ì›Œë“œ ìš°ì„  ì¶”ì¶œ
        noun_patterns = [
            r'[ê°€-í£]{2,}(?:ì•±|ì–´í”Œ|í”Œë«í¼|ì„œë¹„ìŠ¤|ì‚¬ì´íŠ¸)',  # ì„œë¹„ìŠ¤ ê´€ë ¨
            r'[ê°€-í£]{2,}(?:êµì¬|ì±…|ìë£Œ|ê°€ì´ë“œ)',  # í•™ìŠµ ìë£Œ
            r'[ê°€-í£]{2,}(?:ê³„íš|ì¼ì •|ìŠ¤ì¼€ì¤„)',  # ê³„íš ê´€ë ¨
            r'[ê°€-í£]{2,}(?:ì˜ˆì‚°|ë¹„ìš©|ê°€ê²©|ëˆ)',  # ë¹„ìš© ê´€ë ¨
            r'[ê°€-í£]{2,}(?:ë°©ë²•|ë°©ì‹|íŒ|ë…¸í•˜ìš°)',  # ë°©ë²• ê´€ë ¨
        ]
        
        keywords = []
        
        # íŠ¹ìˆ˜ íŒ¨í„´ ë¨¼ì € ì¶”ì¶œ
        for pattern in noun_patterns:
            matches = re.findall(pattern, item)
            keywords.extend(matches)
        
        # ì¼ë°˜ ëª…ì‚¬ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        words = re.findall(r'[ê°€-í£a-zA-Z]{2,}', item)
        for word in words:
            if word not in stopwords and word not in keywords:
                keywords.append(word)
        
        return keywords[:5]  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ
    
    def _generate_item_specific_queries(self, keywords: List[str], context: str = "") -> List[str]:
        """í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        if not keywords:
            return []
        
        main_keyword = keywords[0]
        additional_keywords = " ".join(keywords[1:3]) if len(keywords) > 1 else ""
        
        # ê²€ìƒ‰ íŒ¨í„´ í…œí”Œë¦¿ (ë²”ìš©ì )
        query_patterns = [
            f"{main_keyword} ë°©ë²• ì¶”ì²œ",
            f"{main_keyword} ê°€ì´ë“œ íŒ", 
            f"{main_keyword} {additional_keywords} ì •ë³´".strip(),
        ]
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê°œì¸í™”
        if context and len(context) > 5:
            context_short = context[:30]  # ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ
            query_patterns.append(f"{main_keyword} {context_short} ì¶”ì²œ")
        
        return query_patterns[:2]  # ì•„ì´í…œë‹¹ ìµœëŒ€ 2ê°œ ì¿¼ë¦¬
    
    def _extract_answer_context(self, answers: List[Dict[str, Any]]) -> str:
        """ë‹µë³€ì—ì„œ ê²€ìƒ‰ì— ìœ ìš©í•œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìœ ì—°í•œ ë°©ì‹)"""
        meaningful_answers = []
        
        for answer_item in answers:
            answer = answer_item.get("answer", "")
            
            if isinstance(answer, list):
                answer = " ".join(answer)
            
            # ì˜ë¯¸ìˆëŠ” ë‹µë³€ í•„í„°ë§ (ì¼ë°˜ì ì¸ ì¡°ê±´ë“¤)
            if self._is_meaningful_answer(answer):
                meaningful_answers.append(answer.strip())
        
        # ë‹µë³€ ê¸¸ì´ì™€ êµ¬ì²´ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê¸´ ë‹µë³€ì´ ë” êµ¬ì²´ì ì¼ ê°€ëŠ¥ì„±)
        meaningful_answers.sort(key=len, reverse=True)
        
        # ìƒìœ„ ë‹µë³€ë“¤ì„ ì¡°í•© (ìµœëŒ€ 3ê°œ)
        selected_answers = meaningful_answers[:3]
        final_context = " ".join(selected_answers)
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ì— ì í•©í•œ ê¸¸ì´ë¡œ ì¡°ì •
        if len(final_context) > 120:
            final_context = final_context[:117] + "..."
        
        return final_context
    
    def _is_meaningful_answer(self, answer: str) -> bool:
        """ë‹µë³€ì´ ì˜ë¯¸ìˆëŠ” ì»¨í…ìŠ¤íŠ¸ì¸ì§€ íŒë‹¨"""
        if not answer or len(answer.strip()) < 2:
            return False
        
        answer = answer.strip()
        
        # ëª…ë°±íˆ ì˜ë¯¸ì—†ëŠ” ë‹µë³€ë“¤ ì œì™¸
        meaningless_patterns = [
            # ë‹¨ì¼ ë¬¸ìë‚˜ ê¸°í˜¸
            r'^[ã„±-ã…ã…-ã…£]$',  # ë‹¨ì¼ í•œê¸€ ììŒ/ëª¨ìŒ
            r'^[!@#$%^&*()_+\-=\[\]{}|;:,.<>?]$',  # ë‹¨ì¼ íŠ¹ìˆ˜ë¬¸ì
            r'^\d+$',  # ìˆ«ìë§Œ
            # ë¬´ì˜ë¯¸í•œ ë°˜ë³µ
            r'^(.)\1{2,}$',  # ê°™ì€ ë¬¸ì 3ë²ˆ ì´ìƒ ë°˜ë³µ
            # ì„ì‹œ/ë¹ˆ ë‹µë³€ íŒ¨í„´
            r'^(ì—†ìŒ|ì—†ë‹¤|ëª¨ë¦„|ì˜ëª¨ë¦„|í•´ë‹¹ì—†ìŒ|íŒ¨ìŠ¤)$',
            r'^(.|_|-|\s)*$',  # íŠ¹ìˆ˜ë¬¸ìë‚˜ ê³µë°±ë§Œ
        ]
        
        import re
        for pattern in meaningless_patterns:
            if re.match(pattern, answer, re.IGNORECASE):
                return False
        
        # ìµœì†Œ ê¸¸ì´ ì²´í¬ (ë„ˆë¬´ ì§§ì€ ë‹µë³€ ì œì™¸)
        if len(answer) < 3:
            return False
        
        # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì²´í¬
        meaningful_chars = re.findall(r'[ê°€-í£a-zA-Z0-9]', answer)
        if len(meaningful_chars) < 2:
            return False
        
        return True
    
    async def enhance_checklist_with_search(
        self, 
        base_checklist: List[str], 
        search_results: List[SearchResult]
    ) -> List[str]:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë³´ê°•"""
        
        if not search_results:
            return base_checklist
        
        # ì„±ê³µì ì¸ ê²€ìƒ‰ ê²°ê³¼ë§Œ í•„í„°ë§
        successful_results = [r for r in search_results if r.success and r.content]
        
        if not successful_results:
            logger.warning("No successful search results to enhance checklist")
            return base_checklist
        
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìœ ìš©í•œ ì •ë³´ ì¶”ì¶œ
        enhancement_info = []
        for result in successful_results:
            # ì§§ê³  ì‹¤ìš©ì ì¸ íŒ ì¶”ì¶œ
            tips = self._extract_actionable_tips(result.content)
            enhancement_info.extend(tips)
        
        # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
        unique_enhancements = list(set(enhancement_info))
        quality_enhancements = [tip for tip in unique_enhancements if len(tip) > 10 and len(tip) < 100]
        
        # ê¸°ì¡´ ì²´í¬ë¦¬ìŠ¤íŠ¸ì™€ ë³‘í•©
        enhanced_checklist = base_checklist.copy()
        
        # ìœ ìš©í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ì²´í¬ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        for enhancement in quality_enhancements[:5]:  # ìµœëŒ€ 5ê°œ ì¶”ê°€
            if not any(enhancement.lower() in item.lower() for item in enhanced_checklist):
                enhanced_checklist.append(f"ğŸ’¡ {enhancement}")
        
        logger.info(f"Enhanced checklist: {len(base_checklist)} -> {len(enhanced_checklist)} items")
        return enhanced_checklist
    
    def _extract_actionable_tips(self, content: str) -> List[str]:
        """ì½˜í…ì¸ ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ íŒ ì¶”ì¶œ"""
        tips = []
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = content.split('. ')
        
        for sentence in sentences:
            sentence = sentence.strip()
            
            # ì‹¤í–‰ ê°€ëŠ¥í•œ íŒì˜ íŠ¹ì§•ì„ ê°€ì§„ ë¬¸ì¥ ì°¾ê¸°
            actionable_keywords = ['ì¶”ì²œ', 'í•„ìš”', 'ì¤€ë¹„', 'í™•ì¸', 'ì˜ˆì•½', 'êµ¬ë§¤', 'ì‹ ì²­', 'ë°©ë¬¸', 'ì—°ë½']
            
            if any(keyword in sentence for keyword in actionable_keywords):
                if 20 <= len(sentence) <= 80:  # ì ì ˆí•œ ê¸¸ì´
                    tips.append(sentence)
        
        return tips[:3]  # ìµœëŒ€ 3ê°œ íŒ

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
perplexity_service = PerplexityService()